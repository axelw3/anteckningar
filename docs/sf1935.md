# SF1935 Sannolikhetsteori och statistik med tillämpning inom maskininlärning 7,5 hp
|Tillfälleskod|Termin(er)|Period(er)|Föreläsare|
|-|-|-|-|
|60216|VT2024|4|Boualem Djehiche|

## Föreläsning 1
### Sannolikhetsteori
Resultatet av ett slumpmässigt försök kallas för ett utfall och betecknas $\omega$.

> **Sannolikhetsrum:**<br>
> $\Omega$ - Utfallsrummet (alla möjliga utfall). Kan vara *diskret* (ändligt eller uppräkneligt oändligt) eller *kontinuerligt*.<br>
> $A$ - En händelse (*mängd av utfall*). $A\sube \Omega$<br>
> $P(A)$ - Sannolikheten att $A$ inträffar.<br>
> $P(A\cap B)$ - Sannolikheten att A *och* B inträffar<br>
> $P(A\cup B)$ - Sannolikheten att A *och/eller* B inträffar<br>
> $P(A^*)=P(\Omega\backslash A)$ - Sannolikheten att A *inte* inträffar

Händelser sägs vara *parvis oförenliga* om de omöjligt kan inträffa samtidigt (dvs. $A_i\cap A_j=\empty\mid i\neq j$; mängderna är disjunkta).

> **Mängdlära:**
> 
> $$(A\cap B)^*=A^*\cup B^*\implies\left(\bigcap_{j=1}^nA_j\right)^*=\bigcup_ {j=1}^nA_j^*\\(A\cup B)^*=A^*\cap B^*\implies\left(\bigcup_{j=1}^nA_j\right)^*=\bigcap_ {j=1}^nA_j^*$$

$P:\mathcal P(\Omega)\to[0,1]$ där $\mathcal P(\Omega)=$ "alla delmängder till $\Omega$"

### Kolmogorovs axiom
1. För varje händelse $A$ gäller att $0\leq P(A)\leq 1$.
2. $P(\Omega)=1$
3. *Additionsformeln:* Om $A_1,A_2,\dots$ är en ändligt eller uppräkneligt oändlig följd av *parvis oförenliga händelser* gäller att $P(A_1\cup A_2\cup\dots)=P(A_1)+P(A_2)+\dots$

Ur dessa kan härledas t.ex.
> *Komplementsatsen:* $P(A^*)=1-P(A)$
> eftersom $P(A^*\cup A)=P(A^*)+P(A)=1$

För godtyckliga händelser (jämf. principen om exklusion/inklusion):
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$

vilket ger *Booles olikhet*:
$P(A\cup B)\leq P(A)+P(B)$

### Sannolikheter i diskreta utfallsrum
*Likformigt sannolikhetsmått:* $P(\omega_i)=\frac1{|\Omega|}$ för alla utfall $\omega_i$
Detta ger $P(A)=\frac{|A|}{|\Omega|}=\frac gm$, där $g$ är antalet *gynnsamma utfall* och $m$ är antalet *möjliga utfall*

### Likformigt sannolikhetsmått och kombinatorik

> **Multiplikationsprincipen:**
> Om åtgärd 1 kan utföras på $a_1$ sätt och åtgärd 2 på $a_2$ sätt, så finns det $a_1a_2$ sätt att utföra båda åtgärderna.


### Betingad sannolikhet
$P(A\mid B)=\frac{P(A\cap B)}{P(B)}$
> **Beyes sats:** $P(A\mid B)P(B)=P(B\mid A)P(A)$


## Föreläsning 2

> **Binomialtal:** Antalet delmängder av $\{1,2,\dots,n\}$ av storlek $k$ betecknas $n\choose k$

### På hur många sätt kan man välja $k$ element av $n$ möjliga?
||Återläggning|Ej återläggning|
|-|-|-|
|Med ordning|$n^k$|$P(n,k)=\frac{n!}{(n-k)!}$|
|Utan ordning||${n\choose k}=\frac{n!}{k!(n-k)!}$ *("binomialtal")*|

> **Exempel:** Dragning *utan återläggning* ur en urna.<br>
> I urnan finns $v$ vita och $s$ svarta kulor. Man drar $n$ kulor ur urnan.<br>
> Hur stor är sannolikheten att man får exakt $k$ vita kulor?
> 
> Möjliga utfall: ${v+s\choose n}$<br>
> Gynnsamma utfall: Vi tar $k$ kulor bland de $v$ vita kulorna, och resterande $n-k$ bland de $s$ svarta kulorna. Detta ger ${v\choose k}\cdot{s\choose n-k}$ *gynnsamma* utfall
> 
> **Svar:** Sannolikheten är $\frac{{v\choose k}\cdot{s\choose n-k}}{v+s\choose n}$.

<!-- -->
> **Exempel:** Sannolikhet vid dragning *med återläggning*.<br>
> Anta att vi tar $n$ kulor ur en urna med $v$ st vita och $s$ st svarta kulor. Hur stor är sannolikheten att vi tar upp *exakt $k$ st vita kulor*?
> 
> Möjliga utfall: $(v+s)^n$<br>
> Gynnsamma utfall: ${n\choose k}v^ks^{n-k}$
> 
> **Svar:** Sannolikheten är $\frac{{n\choose k}v^ks^{n-k}}{(v+s)^n}$.

### Oberoende händelser
Om $A$ och $B$ är oberoende $\iff P(A\mid B)=P(A)\iff P(A\cap B)=P(A)\cdot P(B)$

> **Härledning av ekvivalens #2:**
> Från tidigare, och definitionen ovan, vet vi att $P(A)=P(A\mid B)=\frac{P(A\cap B)}{P(B)}\\\iff P(A\cap B)=P(A)\cdot P(B)$

### Betingade sannolikheter
> Om den givna händelsen är densamma gäller:
> 
> 1. $P(B|B)=1$
> 2. $P(A^*|B)=1-P(A|B)$
> 3. (om $A_1,A_2,\dots,A_n$ *disjunkta*) $P(\bigcup_{j=1}^nA_j|B)=\sum_{j=1}^nP(A_j|B)$

<!-- -->
> **Satsen om total sannolikhet:**
> Om $\Omega=\bigcup_{i=1}^\infty H_i$ och $H_i\cap H_j=\empty\mid i\neq j$ gäller att:<br>
> $P(A)=\sum_{i=1}^\infty P(A|H_i)P(H_i)$

Vi får därför även:
$P(H_j|A)=\frac{P(H_j\cap A)}{P(A)}=\frac{P(H_j\cap A)}{\sum_{k=1}^\infty P(A|H_k)P(H_k)}$

> **Exempel:** Covid-19, diagnostiskt test<br>
> *Händelser $A$ = "personen har sjukdomen", $B$ = "positivt test"<br>
> Låt t.ex. $P(B|A)=90\%$, $P(B^*|A^*)=99,532\%$ och $P(A)=0,3\%$<br>
> Hur stor är sannolikheten att en person har sjukdomen, givet ett positivt testsvar*?
> 
> **Lösning:**<br>
> $P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}$<br>
> $P(B)=P(B\cap A)+P(B\cap A^*)=P(B|A)P(A)+P(B|A^*)P(A^*)\\=0,90\cdot0,003+(1-0,99532)\cdot(1-0,003)=0,0027+0,00466596=0,00736596$
> 
> Vilket ger<br>
> $P(A|B)=\frac{P(B|A)P(A)}{P(B)}=\frac{0,90\cdot0,003}{0,00736596}\approx0,366551=36,6551\%$
> 
> **Svar:** 36,6551% sannolikhet


### Oändliga utfallsrum
> Om $\Omega$ är *uppräkneligt eller överuppräkneligt oändlig* har vi ett *oändligt utfallsrum*.

Vi skapar en avbildning $X:\Omega\to\mathbb{R}$.
> $X$ är en stokastisk variabel om det, för varje $C\sube \mathbb{R}$, gäller att $X^{-1}(C)\sub\Omega$.<br>
> *Obs! $X^{-1}$ behöver inte vara inversen till $X$*.

<!-- -->
> **Exempel:** Kast av två tandpetare.<br>
> Vinkeln mellan tandpetarna är $\theta\in[0,\pi]$.<br>
> Låt $X=\theta$, dvs. $\omega\mapsto X(\omega)=\theta=\text{"vinkeln mellan tandpetarna"}$

Vi har ett annat sannolikhetsmått här:
$P(A)=P(\{\omega\mid X(\omega)\in C\})=\mu(C)$

## Föreläsning 3
*Inspelad föreläsning 3-4 av Björn-Olof Skytt.*

**Stokastisk variabel:** Varje utfall är ett tal.<br>
En stokastisk variabel betecknas med stor bokstav, t.ex. $X$.<br>
Ett utfall betecknas med liten bokstav, t.ex. $x,k$.

En stokastisk variabel kan vara **diskret** ($X$ antar ett uppräkneligt antal olika värden) eller **kontinuerlig**.

Tre viktiga funktioner:

* Sannolikhetsfunktion (endast diskret)
* Täthetsfunktion (endast kontinuerlig)
* Fördelningsfunktion (båda fallen)

### Diskret stokastisk variabel
En diskret stokatisk variabel $X$ kan uppta ett ändligt, eller uppräkneligt oändligt, antal värden.
> En **sannolikhetsfunktion** betecknas $p_X(x)$.<br>
> $p_X(x)=P(X=x)=$ *"sannolikheten att $X$ antar värdet $x$"*<br>
> $p_X:X(\Omega)\to[0,1]$

När sannolikhetsfunktionen söks så skall $p_X(x)$ anges för alla $x$.<br>
$p_X(x)$ ritas upp med ett stolpdiagram:

<center><svg width="150" height="120"><path d="M20,0v100h130v-2h-128v-98zM17,11h3v2h-3zM17,46h3v2h-3zM17,81h3v2h-3z"/><path d="M34,98v-16h5v16zm25,0v-50h5v50zm25,0v-80h5v80zm25,0v-55h5v55zm25,0v-50h5v50z"/><text x="0" y="15" font-size="10px">0,3</text><text x="0" y="50" font-size="10px">0,2</text><text x="0" y="85" font-size="10px">0,1</text><text x="37" y="112" font-size="10px" text-anchor="middle">1</text><text x="61" y="112" font-size="10px" text-anchor="middle">2</text><text x="87" y="112" font-size="10px" text-anchor="middle">3</text><text x="112" y="112" font-size="10px" text-anchor="middle">4</text><text x="137" y="112" font-size="10px" text-anchor="middle">5</text></svg></center>

Summan av alla sannolikheter är 1.

$$\sum_{\text{alla utfall}\ x}p_X(x)=1$$

> **Exempel:** Kasta tärning<br>
> Etta$\implies$vinst 1kr<br>
> Tvåa, trea$\implies$vinst 2kr<br>
> Fyra, femma, sexa$\implies$vinst 4kr<br>
> Låt då $X=\text{"vinsten i kr"}$
> 
> Observera att utfallsrummet $\Omega=\{1,2,4\}$<br>
> Sannolikhetsfunktionen definieras då av:<br>
> $p_X(1)=\frac16$, $p_X(2)=\frac26$ och $p_X(4)=\frac36$

#### Fördelningsfunktionen för $X$ (*i det diskreta fallet*)
**Definition:** $F_X(x)=P(X\leq x)$

I det diskreta fallet blir $F_X(x)=\sum_{\text{alla utfall}\ \leq x}p_X(x)$.
> I fallet med tärningskast får vi<br>
> $F_X(1)=p_X(1)=\frac16$<br>
> $F_X(2)=p_X(1)+p_X(2)=\frac16+\frac26=\frac36$<br>
> $F_X(4)=p_X(1)+p_X(2)+p_X(4)=1$

För en fördelningsfunktion $F_X(x)$ gäller generellt att:
> $0\leq F_X(x)\leq 1$<br>
> $F_X(-\infty)=P(X\leq -\infty)=0$<br>
> $F_X(\infty)=P(X\leq \infty)=1$<br>
> $F_X(x)$ avtar aldrig

#### Olika diskreta fördelningar (*finns i formelsamlingen*)
*Dessa står i formelsamlingen (förutom de 2 första).*<br>
Nedan beskrivs fördelningarna av deras *sannolikhetshetsfunktioner*.

* **Enpunktsfördelning:** Hela massan för $X$ är koncentrerad i ett enda värde $a$, dvs.

$$p_X(a)=1$$

* **Tvåpunktsfördelning:** Den stokastiska variabeln $X$ antar endast två värden $a,b$, och $p_X(a)=p, p_X(b)=1-p$.
	* **Bernoullifördelningen:** $X$ antar $0$ eller $1$<br>
	$\Omega=\{0,1\}$<br>
	$p_X(1)=p,\ p_X(0)=1-p$
* ***Diskret* likformig fördelning:** Här är varje möjligt utfall lika sannolikt. $X$ antar värdena $1,2,\dots,m$ och $p_X(k)=\frac1m,\ k=1,2,\dots,m$. T.ex. vanligt tärningskast.
* **För-första-gångenfördelningen (*eng. "geometric distribution"*):** Här är $X="\text{antal försök }\textit{t.o.m. det första lyckade}"$.

Vi antar här att vi i varje försök har samma sannolikhet $p$ att lyckas, och att $A_i$ är händelsen "lyckas precis i försök nr $i$. Det ger:<br>
$p_X(1)=P(X=1)=P(A_1)=p\\p_X(2)=P(X=2)=P(A_1^*\cap A_2)=(1-p)p\\p_X(3)=P(X=3)=P(A_1^*\cap A_2^*\cap A_3)=(1-p)^2p$<br>
Dvs. allmänt:

$$p_X(k)=(1-p)^{k-1}p$$

om $X\in\text{ffg}(p)$

* **Geometrisk fördelning (*obs skillnad mellan eng. & sv*):** 
$X=\text{"antal misslyckade försök \textit{innan} vi lyckas"}$

$$p_X(k)=(1-p)^kp$$

* **Binomialfördelningen:** Vi gör $n$ st försök och antar att sannolikheten att lyckas är $p$ (dvs. *samma* varje enskild gång).

$X=\text{"totalt antal lyckade försök"}$.<br>
Vi skriver då att $X\in \text{Bin}(n,p)$.<br>
Allmänt, när $X\in \text{Bin}(n,p)$:

$$p_X(k)={n\choose k}p^k(1-p)^{n-k}$$

> **Exempel:** Vi gör 7 st försök. Vad är $P(X=3)$ om $X\in \text{Bin}(7,p)$?
> En variant: $P(\circ\bullet\bullet\circ\circ\bullet\circ)=p^3(1-p)^4$
> En annan: $P(\bullet\circ\circ\circ\bullet\circ\bullet)=p^3(1-p)^4$
> Antal varianter (sätt att välja 3 av 7): $7\choose 3$
> Dvs. totalt $P(X=3)={7\choose3}p^3(1-p)^4$

* **Hypergeometrisk fördelning:** Vi drar utan återläggning.

Från början har vi $N$ st enheter, där andelen med egenskap $A$ är $p$. Sedan drar vi $n$ st enheter, utan återläggning.<br>
$X="\text{antal enheter vi får med egenskap}\ A"$<br>
$p_X(k)={n\choose k}\Pi_{j=0}^{n-k-1}\frac{Np}{N-j}\Pi_{j=0}^{k-1}\frac{Np-j}{N-n+k-j}$

Vi säger att $X\in\text{Hyp}(N,n,p)$, och då är

$$p_X(k)=\frac{{Np\choose k}{N(1-p)\choose n-k}}{N\choose n}$$

* **Poisson-fördelningen:** Vi har ett tidsintervall $[0,t]$ och antar att det i varje tidpunkt är lika stor sannolikhet att en händelse inträffar (oberoende av om det redan inträffat eller ej).

Låt den s.v. $X="\text{antal händelser som inträffar på }[0,t]"$<br>
Vi antar att händelserna inträffar med intensiteten $\lambda=\frac{\text{händelser}}{\text{tidsenhet}}$.

$$p_X(k)=\frac{(\lambda t)^k}{k!}e^{-\lambda t}$$

I formelsamlingen har man fixerat tiden $t$ och låter istället<br>
$\mu=\lambda t$ (*antal händelser i genomsnitt under tidsintervallet*):

$$p_X(k)=\frac{\mu^k}{k!}e^{-\mu}$$

Vi säger att $X\in P_o(\mu)$

### Kontinuerlig stokastisk variabel
Oändligt många utfall som ligger oändligt tätt. Dessutom går sannolikheten för varje utfall mot 0 (finns därmed ingen sannolikhetsfunktion).

#### Täthetsfunktioner
Betecknas $f_X(x)$
> Definition: $\int_a^bf_X(x)dx=P(a\lt X\leq b)$

Fördelningsfunktionen $F_X(x)$ kan då definieras som

$$F_X(x)=\int_{-\infty}^xf_X(t)dt\\\iff f_X(x)=\frac d{dx}F_X(x)$$

#### Kontinuerliga fördelningar
* **Likformig (*kontinuerlig*) fördelning:** "Rektangelfördelningen"

Täthetsfunktionen är konstant på det intervall $[a,b]$ där $X$ kan hamna:
$\int_a^b f_X(x)dx=1\implies\int_a^b kdx=1\iff k=\frac 1{b-a}$

$$f_X(k)=\begin{cases}\frac 1{b-a}&k\in(a,b)\\0&\text{annars}\end{cases}$$

Detta skrivs $X\in\text U(a,b)$
T.ex. En buss går var 10:e minut. Vi går slumpmässigt till hållplatsen. $X=$ "minuter vi får vänta" $\in\text U[0,10]$

* **Exponentialfördelningen:**

Låt oss börja med poissonfördelningen $p_X(k)=\frac{\mu^k}{k!}e^{-\mu}$.

> Anta att vi har ett intervall $[0,t]$, och att det är lika sannolikt att en händelse inträffar *i varje given punkt*, oberoende av huruvida den har inträffat tidigare.<br>
> Låt då $X=$ "antalet händelser som inträffar på intervallet"<br>
> $\implies X\in P_o(\lambda t)$ där $\lambda=\frac{\text{händelser}}{\text{enhet}}$

Om $X\in P_o(\lambda t)$ och $T$ är tiden tills nästa händelse inträffar, så gäller att

$$T\in\text{exp}(\lambda)$$

> **Bevis:** Vi vill ha $f_T(t)$ när vi har $p_X(x)$.
> 
> Ta först fram<br>
> $F_T(t)=P(T\leq t)=P("\text{nästa händelse inträffar innan }t")\\=1-P("\text{nästa händelse inträffar efter }t")\\=1-P("\text{0 händelser innan t}")=1-p_X(0)\\=1-\frac{(\lambda t)^0}{0!}e^{-\lambda t}\\=1-e^{-\lambda t}$
> 
> $\implies$täthetsfunktionen $f_T(t)=\frac d{dt}F_T(t)=\lambda e^{-\lambda t},\ t\gt0$

Allmänt ges täthetsfunktionen av:

$$f_X(x)=\begin{cases}\lambda e^{-\lambda x}&x\gt0\\0&\text{annars}\end{cases}$$

**Obs!** Exponentialfördelningen saknar minne:
> Om en lampas brinntid är $T\in\text{exp}(\lambda)$, och att vi räknar tiden fr.o.m. att förra lampan slocknade. Då blir $P(T>100)=\int_{100}^\infty f_T(t)dt=e^{-100\lambda}$.
> 
> Anta nu att vi istället räknar tiden sedan vi gick in i rummet:<br>
> Även i detta fall blir $P(T>100)=e^{-100\lambda}$.
> 
> **Bevis:**<br>
> Vi vill visa att $P(T>t)=P(T>t+x| T>x)$.<br>
> $P(T>t+x|T>x)=\frac{P(T>t+x\ \cap\ T>x)}{P(T>x)}=\frac{P(T>t+x)}{P(T>x)}\\=\frac{e^{-(t+x)\lambda}}{e^{-x\lambda}}=e^{-t\lambda}=P(T>t)$

* **Normalfördelningen:**

$$f_X(x)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

#### Blandad fördelning
> **Exempel 3.14**<br>
> Vi har ett trafikljus som växlar mellan rött och grönt ($a$ sek vardera).<br>
> $X="\text{antal sekunder vi får vänta}"$
> 
> Lagen om total sannolikhet ger fördelningsfunktionen:<br>
> $F_X(x)=P(X\leq x)\\=P("\text{behöver vänta högst }x\text{ sekunder}")\\=P(X\leq x|\text{grönt})\cdot P(\text{grönt})+P(X\leq x|\text{rött})\cdot P(\text{rött})\\=P(\text{grönt})+P(X\leq x|\text{rött})\cdot P(\text{rött})\\=\frac12+\frac12\cdot\int_0^x\frac1adt=\frac12+\frac x{2a}$ (*för $0\lt x\lt a$*)

#### Funktioner av stokastiska variabler
> **Ett diskret exempel:**
> 
> |$j$|$-3$|$-2$|$-1$|$0$|$1$|$2$|$3$|
> |-|-|-|-|-|-|-|-|
> |$p_X(j)$|$0.02$|$0.08$|$0.15$|$0.50$|$0.15$|$0.08$|$0.02$|
> 
> Låt nu t.ex. $Y=X^2$. Vi söker $p_Y(k)$<br>
> $Y$ kan anta värdena $0,1,4,9$. Vi får:
> > $p_Y(0)=P(X=0)=0.50\\p_Y(1)=P(X=\pm1)=0.30\\p_Y(4)=P(X=\pm2)=0.16\\p_Y(9)=P(X=\pm3)=0.04$

<!-- -->
> (3.19) **Ett kontinuerligt exempel:**<br>
> Givet: Funktion $X\in\text U[0,1]$ och $Y=-\frac1\lambda\ln X$<br>
> $f_X(k)=\begin{cases}1&k\in[0,1]\\0&\text{annars}\end{cases}$<br>
> $F_Y(y)=P(Y\leq y)=P(X\geq e^{-\lambda y})=\int_{e^{-\lambda y}}^\infty f_X(t)dt\\=\int_{e^{-\lambda y}}^11dt=1-e^{-\lambda y}$<br>
> $\implies f_Y(y)=\frac d{dy}F_Y(y)=\lambda e^{-\lambda y}$

## Föreläsning 4
### Flerdimensionella stokastiska variabler
Fördelningsfunktioner:

$$F_{X,Y}(x,y)=P(X\leq x\cap Y\leq y)$$

#### En dimension, diskret
Sannolikhetsfunktion $p_X(x)$

> $p_X(x)$ kallas för **marginell sannolikhetsfunktion**

#### Två dimensioner, diskret
T.ex. kasta två tärningar samtidigt.<br>
$p_{X,Y}(x,y)=P(X=x\cap Y=y)$
> $\sum_{\text{alla utfall }x,y}p_{X,Y}(x,y)=1$

> $p_{X,Y}(x,y)$ kallas för **simultan sannolikhetsfunktion**

Givet den simultana sannolikhetsfunktionen kan den *marginella* sannolikhetsfunktionen fås av<br>
$p_X(x)=\sum_{\text{alla utfall }y}p_{X,Y}(x,y)\\p_Y(y)=\sum_{\text{alla utfall }x}p_{X,Y}(x,y)$

Om $X$ och $Y$ är oberoende<br>
$p_{X,Y}(x,y)=P(X=x\cap Y=y)\\=\text{\{om oberoende\}}=p_X(x)p_Y(y)$

#### En dimension, kontinuerlig
Täthetsfunktion $f_X(x)$<br>
$\int_{-\infty}^\infty f_X(x)dx=1$

#### Två dimensioner, kontinuerlig
Täthetsfunktion $f_{X,Y}(x,y)$<br>
Låt $D=\{\text{en mängd av par }(x,y)\}$

> Obs! flervariabel dubbelintegral? I föreläsningen 18:00

$f_X(x)=\int_{-\infty}^\infty f_{X,Y}(x,y)dy\\f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)dx$

Om oberoende:<br>
$f_{X,Y}(x,y)=\text{\{om oberoende\}}=f_X(x)f_Y(y)$

#### Max- och min-problem
Vi antar att $F_X(x)$ och $F_Y(y)$ är givna, och att $X$ och $Y$ är oberoende.<br>
$Z=\max(X,Y)$<br>
$F_Z(z)=P(Z\leq z)=P(\max(X,Y)\leq z)\\=P(X\leq z\cap Y\leq z)=\text{\{oberoende enligt uppgift\}}\\=P(X\leq z)\cdot P(Y\leq z)=F_X(z)F_Y(z)$

Låt istället $Z=\min(X,Y)$<br>
$F_Z(z)=P(Z\leq z)=P(\min(X,Y)\leq z)\\=P(X\leq z\cup Y\leq z)\\=P(X\leq z)+P(Y\leq z)-P(X\leq z\cap Y\leq z)\\=\text{\{likt ovan\}}=F_X(z)+F_Y(z)-F_X(z)F_Y(z)$

#### Summa $Z=X+Y$ (där $X,Y$ oberoende)
Anta att vi har givna sannolikhetsfunktioner $p_X(x),p_Y(y)$ och att $Z=X+Y$. Vi söker nu $p_Z(z)$.

$$p_Z(z)=\sum_{\substack{\text{alla }x,y\\\text{s.a. }x+y=z}}p_{X,Y}(x,y)=\sum_{j=0}^zp_{X,Y}(j,z-j)$$

> **Exempel:** Anta att $X\in P_o(\mu_1),Y\in P_o(\mu_2)$ och $X,Y$ oberoende.<br>
> $p_X(k)=\frac{\mu_1^k}{k!}e^{-\mu_1}$<br>
> $p_Y(k)=\frac{\mu_2^k}{k!}e^{-\mu_2}$<br>
> Bilda $Z=X+Y$ och ta fram $p_Z(k)$.<br>
> $p_Z(k)=P(Z=k)=P(X+Y=k)\\=\sum_{j=0}^kp_{X,Y}(j,k-j)=\text{\{oberoende\}}\\=\sum_{j=0}^kp_X(j)\cdot p_Y(k-j)=\sum_{j=0}^k\frac{\mu_1^j}{j!}e^{-\mu_1}\frac{\mu_2^j}{j!}e^{-\mu_2}\\=\sum_{j=0}^k\frac{\mu_1^j\mu_2^{k-j}}{j!\cdot(k-j)!}e^{-\mu_1-\mu_2}=\frac1{k!}e^{-\mu_1-\mu_2}\sum_{j=0}^k{k\choose j}\mu_1^j\mu_2^{k-j}\\=\frac{(\mu_1+\mu_2)^k}{k!}e^{-(\mu_1+\mu_2)}$<br>
> Dvs. $Z\in P_o(\mu_1+\mu_2)$

<!-- -->
> **Viktig sats:** Om $X\in P_o(\mu_1),Y\in P_o(\mu_2)$ och $X,Y$ oberoende så är $X+Y\in P_o(\mu_1+\mu_2)$.

#### Summa, kontinuerliga fallet
Vi antar att $Z=X+Y$.<br>
$F_Z(z)=P(X+Y\leq z)={\int\int}_{x+y\leq z}f_{X,Y}(x,y)dxdy=\text{\{oberoende\}}={\int\int}_{x+y\leq z}f_{X}(x)f_Y(y)dxdy=\int_{-\infty}^\infty\int_{-\infty}^{z-x}f_X(x)f_Y(y)dydx$


## Föreläsning 5
### Kapitel 5
> **Exempel:** Tärningskast där<br>
> 1 $\implies$ 1 kr<br>
> 2,3 $\implies$ 2 kr<br>
> 4,5,6 $\implies$ 4 kr<br>
> Vi kastar tärning 6000 gånger. Vi borde då vinna totalt $1\cdot\frac166000+2\cdot\frac266000+4\frac366000=1000+4000+12000=17000$ och *i genomsnitt* $\frac{17000}{6000}=\frac{17}{6}$.

**Väntevärdet** av $X$ betecknas $\mu=E(X)$ och är ett lägesmått av vad $X$ blir *i genomsnitt* vid oändligt många försök.

$$E(X)=\sum_{\text{alla utfall }x}x\cdot p_X(x)\\=\text{\{i kontinuerliga fallet\}}=\int_{-\infty}^\infty x\cdot f_X(x)dx$$

För funktioner av $X$:

$$E(g(X))=\sum_{\text{alla utfall }x}g(x)p_X(x)=\int_{-\infty}^\infty g(x)\cdot f_X(x)dx$$

> **Exempel:** Vad blir $E(X^2)$ i tärningsexemplet?<br>
> $E(X^2)=\sum_{\text{alla utfall }x}x^2p_X(x)=1^2p_X(1)+2^2p_X(2)+4^2p_X(4)=1\frac16+4\frac26+16\frac36=\frac{57}{6}=\frac{19}{2}$

Observera att $E(x^2)\neq E^2(X)$ generellt.

*Standardbeteckningen för väntevärde*: $\mu$

**Variansen** av $X$ betecknas $V(X)$ och är ett spridningsmått.<br>
$V(X)=E((X-\mu)^2)="\text{genomsnittet av kvadrerade avvikelser från }\mu"$

I exemplet ovan<br>
$V(X)=E((X-\mu)^2)=\sum_{\text{alla utfall }x}(x-\mu)^2p_X(x)\\=(1-\frac{17}6)^2\cdot\frac16+(2-\frac{17}6)^2\cdot\frac26+(4-\frac{17}6)^2\cdot\frac36=\frac{53}{36}$

$E((X-\mu)^2)=E(X^2-2X\mu+\mu^2)\\=E(X^2)-E(2X\mu)+E(\mu^2)=E(X^2)-2\mu E(X)+\mu^2\\=E(X^2)-2\mu^2+\mu^2=E(X^2)-\mu^2=E(X^2)-E^2(X)$

Detta ger $V(X)=E(X^2)-E^2(X)$ *(står i formelsamlingen)*

**Standardavvikelse:** $\sigma=D(X)=\sqrt{V(X)}$

**Variationskoefficient:** Ett relativt mått. $R(X)=\frac{D(X)}{E(X)}$

**Median:** $\widetilde{X},\ F_X(\widetilde X)=0.5$

#### Tre viktiga räkneregler
$$(1)\ E(aX+bY+c)=aE(X)+bE(Y)+c$$

$$(2)\ V(aX+b)=E((aX+b)^2)-E^2(aX+b)\\=E(a^2X^2+2aXb+b^2)-E^2(aX+b)\\=a^2E(X^2)+2abE(X)+b^2-(a^2E^2(X)+2abE(X)+b^2)\\=a^2(E(X^2)-E^2(X))=a^2V(X)$$

$$(3)\ \textit{om oberoende: }V(X+Y)=V(X)+V(Y)$$

**Obs!** $V(X-Y)\neq V(X)-V(Y)$ eftersom $V(X)\gt_\text{alltid}0$.

$V(X-Y)=V(X)+V(-Y)=V(X)+V(Y)$

## Föreläsning 6
Innehåll (VT24): 5.4-5.6<br>
*Anteckningar för Björn-Olof Skytts föreläsning 6.*

> **Repetition:**<br>
> Väntevärde för $X$:
> 
> $$E(X)=\mu=\sum_{\text{alla utfall }x}xp_X(x)\\=\int_{-\infty}^\infty xf_X(x)dx$$
> 
> Variansen för $X$:
> $V(X)=\sigma^2=E((X-\mu)^2)=E(X^2)-E^2(X)$
> 
> Standardavvikelsen för $X$:
> $X=D(X)=\sqrt{V(X)}$

### Två dimensioner
$E(g(X,Y))=\sum_{\text{alla utfall }x,y}g(x,y)p_{X,Y}(x,y)\\=\int_{-\infty}^\infty\int_{-\infty}^\infty g(x,y)f_{X,Y}(x,y)dxdy$

### Sambandet mellan $X$ och $Y$
Sambandet kan beskrivas av *kovariansen* och *korrelationskoefficienten* (§ 2 i formelsamling)

**Kovarians:** $C(X,Y)=E((X-\mu_X)(Y-\mu_Y))\\=E(XY)-E(X)E(Y)$

Dvs. $C(X,Y)=\sum_{\text{alla utfall }x,y}(x-\mu_X)(y-\mu_Y)p_{X,Y}(x,y)$

> *Kovariansen av $X$ och $X$ är variansen: $C(X,X)=V(X)$*

**Korrelationskoefficient:** $\rho(X,Y)=\frac{C(X,Y)}{D(X)\cdot D(Y)}$
$-1\leq\rho(X,Y)\leq1$.

<center><svg width="375" height="140"><defs><g id="xy"><path d="M59,10v49h-49v2h49v49h2v-49h49l-2,2l2,2l5,-5l-5,-5l-2,2l2,4v-2h-49v-49l2,2l2,-2l-5,-5l-5,5l2,2z"/><text x="68" y="10" font-size="12px">Y</text><text x="115" y="76" font-size="12px">X</text></g><g id="dots"><path d="M70,35h2v2h-2zM85,40h2v2h-2zM90,30h2v2h-2zm5,10h2v2h-2zM40,65h2v2h-2zM50,70h2v2h-2zM55,55h2v2h-2zM25,90h2v2h-2zM30,80h2v2h-2zM35,70h2v2h-2zM42,85h2v2h-2zM48,45h2v2h-2zM64,64h2v2h-2zM75,54h2v2h-2zM65,25h2v2h-2zM20,74h2v2h-2zM100,15h2v2h-2z"></g></defs>
<use xlink:href="#xy" x="0" y="0"/>
<use xlink:href="#xy" x="125" y="0"/>
<use xlink:href="#xy" x="250" y="0"/>
<use xlink:href="#dots" x="0" y="0"/>
<use xlink:href="#dots" x="-245" y="0" transform="scale(-1,1)"/>
<use xlink:href="#dots" x="-370" y="0" transform="scale(-1,1)"/>
<use xlink:href="#dots" x="250" y="0"/>
<text x="40" y="134">ρ &gt; 0</text><text x="165" y="134">ρ &lt; 0</text><text x="290" y="134">ρ &thickapprox; 0</text></svg></center>

Ju *starkare* sambandet är, desto närmare är $\rho=\pm1$.<br>
Om $\rho(X,Y)=0$ kallas $X,Y$ för *okorrelerade*.

Speciellt är korrelationskoefficienten för $X$ och $X$:

$$\rho(X,X)=\frac{C(X,X)}{D(X)D(X)}=\frac{V(X)}{V(X)}=1$$

Om $X,Y$ är *oberoende*, så gäller att<br>
$E(X\cdot Y)=\sum_{\text{alla }x}\sum_{\text{alla }y}x\cdot yp_{X,Y}(x,y)=\{\text{oberoende}\}\\=\sum_{\text{alla }x}\sum_{\text{alla }y}xy\cdot p_X(x)p_Y(y)\\=\sum_{\text{alla }x}xp_X(x)\cdot\sum_{\text{alla }y}yp_Y(y)=E(X)E(Y)$

Detta ger att<br>

$$C(X,Y)=E(X\cdot Y)-E(X)E(Y)=0$$

samt $\rho(X,Y)=0\implies$okorrelerade

> **Sats:**
> Om $X,Y$ är oberoende $\implies X,Y$ okorrelerade
> 
> *Obs! Det omvända gäller inte alltid, t.ex. om<br>
> $X\in\text U[-1,1]$ och $Y=X^2$ fås att*<br>
> $C(X,Y)=C(X,X^2)=E(X^3)-E(X)E(X^2)\\=\int_{-1}^1 x^3\frac12dx-\int_{-1}^1x\frac12dx\cdot\int_{-1}^1x^2\frac12dx=0-0\cdot E(X^2)$<br>
> Alltså är $X,Y$ *okorrelerade*, även om de INTE är *oberoende*.

<!-- -->
> **Övningsuppgift 5.18:**<br>
> En atom som kan hoppa upp, ner, höger eller vänster med lika stor sannolikhet.
> $p_{X,Y}(1,0)=p_{X,Y}(0,1)=p_{X,Y}(-1,0)=p_{X,Y}(0,-1)=\frac14$
> 
> a) Bestäm $\rho(X,Y)$<br>
> $\rho(X,Y)=\frac{C(X,Y)}{D(X)D(Y)}=\frac{E(XY)-E(X)E(Y)}{D(X)D(Y)}$<br>
> Vi har<br>
> $E(X\cdot Y)=\sum_{\text{alla utfall }x,y}xyp_{X,Y}(x,y)=\frac14\cdot0=0$<br>
> $E(X)=\sum_{\text{alla utfall }x,y}xp_{X,Y}(x,y)=p_{X,Y}(1,0)-p_{X,Y}(-1,0)=0$<br>
> $\implies C(X,Y)=0-0\cdot E(Y)=0$<br>
> $\implies\rho(X,Y)=0$
> 
> b) Är $X,Y$ oberoende?
> 
> $$X,Y\text{ oberoende}\\\iff\\P(X=x\cap Y=y)=P(X=x)\cdot P(Y=y)\\\text{för alla }x,y$$
> 
> $(1,0):p_{X,Y}(1,0)=\frac14\neq \frac14\cdot\frac12=p_X(1)\cdot p_Y(0)$<br>
> **Svar:** Dvs. $X,Y$ är **inte oberoende**.

### Räkneregler för kovarianser
Vi kommer ihåg<br>
$V(aX+b)=V(aX)$<br>

För kovarianser:<br>
$C(X,Y+b)=C(X,Y)$

**Allmänt:**
$C(aX+bY,cZ+dW)=ac\cdot C(X,Z)+ad\cdot C(X,W)+bc\cdot C(Y,Z)+bd\cdot C(Y,W)$

$V(X+Y)=C(X+Y,X+Y)=\{\text{enligt ovan}\}\\=C(X,X)+C(X,Y)+C(Y,X)+C(Y,Y)\\=V(X)+V(Y)+2C(X,Y)$<br>
Detta ger, *för oberoende $X,Y\implies C(X,Y)=0$*, den tidigare formulerade regeln:<br>
$V(X+Y)=V(X)+V(Y)$

> **Sammanfattning av räkneregler:**<br>
> $E(aX+bY+c)=aE(X)+bE(Y)+c$<br>
> $V(aX+b)=V(aX)+a^2V(X)$<br>
> $V(X+Y)=V(X)+V(Y)+2C(X,Y)$

### Medelvärde
Anta att $X_1,X_2,\dots,X_n$ är *oberoende* och $E(X_i)=\mu$, $D(X_i)=\sigma$

$X$-medel: $\overline X_n=\frac1n\sum_{i=1}^nX_i$<br>
$E(\overline X_n)=E(\frac1n(X_1+\cdots+X_n))=\frac1nE(X_1+\cdots+X_n)\\=\{\text{lika väntevärden}\}=\frac1nnE(X_i)=\mu$

$V(\overline X_n)=V(\frac1n(X_1+\cdots+X_n))=\frac1{n^2}V(X_1+\cdots+X_n)\\=\frac1{n^2}nV(X_i)=\frac{V(X_i)}n=\frac{\sigma^2}n$<br>
$\implies D(\overline X_n)=\frac\sigma{\sqrt{n}}$

> *Spridningen mellan medelvärdena i olika mätserier är mindre än spridningen mellan mätvärdena i samma serie.*<br>
> Detta visar att man får bättre säkerhet med flera mätningar.

### Stora talens lag
> För alla $\epsilon\gt0$ gäller att<br>
> $P(\mu-\epsilon\lt\overline X_n\lt\mu+\epsilon)\to1$ då $n\to\infty$

## Föreläsning 7
*Anteckningar från Björn-Olof Skytt, föreläsning 7.*<br>
*I boken ur kapitel 5.*

### Några olikheter
$$\begin{gathered}X&=&\Theta&+&\delta&+&\epsilon\\\text{uppmätt}&&\text{korrekt}&&\text{systematiskt}&&\text{slumpmässigt}\\\text{värde}&&\text{värde}&&\text{fel}&&\text{fel}\end{gathered}$$

**Väntevärde:**

$$E(X)=E(\Theta+\delta+\epsilon)=\Theta+\delta=\mu$$

vilket ger $\delta=\mu-\Theta$ och $\epsilon=X-E(X)=X-\mu$

**Noggrannhet och precision:**<br>
<center><svg width="240" height="170"><defs><g id="dart"><circle r="55" cx="0" cy="0" fill="none" stroke="#000"/><circle r="40" cx="0" cy="0" fill="none" stroke="#000"/><circle r="25" cx="0" cy="0" fill="none" stroke="#000"/><circle r="10" cx="0" cy="0" fill="none" stroke="#000"/></g></defs>
<use xlink:href="#dart" x="120" y="60"/>
<path d="M140,30h3v3h-3zm3,-7h3v3h-3zm5,12h3v3h-3zm0,-7h3v3h-3zm-4,4h3v3h-3zm-1,-3h3v3h-3z"/>
<text x="120" y="137" font-size="13px" text-anchor="middle">God precision (litet slumpfel)</text><text x="120" y="159" font-size="13px" text-anchor="middle">Dålig noggrannhet (stort systematiskt fel)</text>
</svg></center>

### Markovs olikhet
$P(Y\geq a)\leq\frac{E(Y)}{a}$ för $a,Y\geq0$
> **Bevis:**<br>
> $E(Y)=\int_0^\infty yf_Y(y)dy=\int_0^a yf_Y(y)dy+\int_a^\infty yf_Y(y)dy\\\geq \int_a^\infty yf_Y(y)dy\geq\left\{\begin{gathered}y\geq a,\text{eftersom }a\\\text{ är den undre gränsen}\end{gathered}\right\}\\\geq a\int_a^\infty f_Y(y)dy=aP(Y\geq a)$<br>
> $\implies E(Y)\geq aP(Y\geq a)\\\implies P(Y\geq a)\leq\frac{E(Y)}a$

### Stora talens lag
$$\lim_{n\to\infty}P(|\bar X_n-\mu|\lt\epsilon)\to1$$

*Dvs: Om man gör tillräckligt många försök så kommer medelvärdet $\bar X_n$ komma så nära väntevärdet $\mu$ som vi önskar.*

> **Bevis:**<br>
> Stora talens lag är ekvivalent med $\lim_{n\to\infty}P(|\bar X_n-\mu|\geq\epsilon)\to0$.<br>
> Vi kvadrerar sannolikhetsuttrycket:<br>
> $P(|\bar X_n-\mu|^2\geq\epsilon^2)\leq\left\{\begin{gathered}\text{enligt Markovs olikhet}\\\text{om }Y=|\bar X_n-\mu|^2\end{gathered}\right\}\\\leq\frac{E(|\bar X_n-\mu|^2)}{\epsilon^2}=\frac{V(\bar X_n)}{\epsilon^2}=\left\{\text{låt }V(X_i)=\sigma^2\right\}=\frac{\sigma^2}{n\cdot\epsilon^2}\\=\left\{\text{då }n\to\infty\right\}\to0$<br>
> Dvs. $P(|\bar X_n-\mu|\geq\epsilon)\to0$<br>
> $\iff P(|\bar X_n-\mu|\lt\epsilon)\to1$

### Tjebysjovs olikhet
*Står även i § 7 i formelsamlingen.*

$$P(|X-\mu|\gt k\sigma)\leq\frac1{k^2}$$

> **Bevis:**<br>
> Vi flyttar $\sigma$ och kvadrerar.<br>
> $P(\frac{|X-\mu|^2}{\sigma^2}\gt k^2)=\left\{\text{Markov}\right\}\leq\frac{E(|X-\mu|^2)}{\sigma^2k^2}=\frac{V(X)}{\sigma^2k^2}=\frac{\sigma^2}{\sigma^2k^2}\\=\frac1{k^2}$<br>
> V.S.V.

### Normalfördelningen
Om $X\in\text N(\mu,\sigma)$:

$$f_X(x)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

$$F_X(x)=\int_{-\infty}^x$$

Om $X\in\text N(\mu,\sigma)$ gäller att

$$Y=\frac{X-\mu}\sigma\in N(0,1)$$

och $Y$ sägs då följa standardnormalfördelningen. Då sägs $f_Y(y)=\phi(y)$ och $F_Y(y)=\Phi(y)$.

<center><svg width="341" height="200"><path stroke="gray" d="M170.5 40v120m29-120v120m29-120v120m29-120v120m29-120v120M141.5 40v120M112.5 40v120M83.5 40v120M54.5 40v120M40 138h261"/><path stroke="#000" d="m305.8 135.7 7.2 2.4-7.2 2.4 1.2-2.4zM168 35.2l2.4-7.2 2.4 7.2-2.4-1.2z"/><path stroke="#000" d="M34 138h273M170.5 34v131.9m0-23.8v-8m29 8v-8m29 8v-8m29 8v-8m29 8v-8m-145 8v-8m-29 8v-8m-29 8v-8m-29 8v-8m120 4h-8"/><text font-size="16" text-anchor="middle" font-family="KaTeX_Main, serif"><tspan x="199.5" y="156">1</tspan><tspan x="228.5" y="156">2</tspan><tspan x="257.5" y="156">3</tspan><tspan x="286.5" y="156">4</tspan><tspan x="141.5" y="156">-1</tspan><tspan x="112.5" y="156">-2</tspan><tspan x="83.5" y="156">-3</tspan><tspan x="54.5" y="156">-4</tspan><tspan x="170.5" y="20.7" font-style="italic" text-anchor="middle">y</tspan><tspan x="320.3" y="143.4" font-style="italic">x</tspan></text><path fill="none" stroke="#00f" d="M32.2 138.1h31l.7-.1h4.6l.7-.1h3.1l.7-.1h1.5l.8-.1h1.5l.8-.1h.7l.8-.1h.7l.8-.1h.7l.8-.1.8-.1h.7l.8-.2h.7l.8-.1.7-.1.8-.1.8-.1.7-.2h.8l.7-.2.8-.2.7-.1.8-.2.7-.1.8-.2.8-.2.7-.2.8-.2.7-.3.8-.2.7-.3.8-.2.8-.3.7-.3.8-.3.7-.4.8-.3.7-.4.8-.4.8-.4.7-.4.8-.5.7-.4.8-.5.7-.5.8-.5.7-.6.8-.6.8-.6.7-.6.8-.6.7-.7.8-.7.7-.7.8-.8.8-.7.7-.8.8-.9.7-.8.8-.9.7-.9.8-1 .8-.9.7-1 .8-1 .7-1 .8-1 .7-1.2.8-1 .7-1.2.8-1.1.8-1.2.7-1.2.8-1.2.7-1.3.8-1.2.7-1.3.8-1.3.8-1.3.7-1.3.8-1.4.7-1.3.8-1.4.7-1.3.8-1.4.8-1.4.7-1.3.8-1.4.7-1.4.8-1.4.7-1.3.8-1.4.7-1.3.8-1.4.8-1.3.7-1.3.8-1.3.7-1.3.8-1.3.7-1.2.8-1.2.8-1.2.7-1.1.8-1.1.7-1.1.8-1 .7-1 .8-1 .8-1 .7-.8.8-.9.7-.8.8-.7.7-.7.8-.7.7-.5.8-.6.8-.5.7-.4.8-.4.7-.3.8-.2.7-.2.8-.2h3l.8.2.7.2.8.3.8.3.7.4.8.5.7.5.8.5.7.6.8.7.7.7.8.8.8.8.7.8.8 1 .7.9.8 1 .7 1 .8 1 .8 1.1.7 1.1.8 1.2.7 1.2.8 1.2.7 1.2.8 1.3.8 1.3.7 1.3.8 1.3.7 1.3.8 1.4.7 1.3.8 1.4.7 1.3.8 1.4.8 1.4.7 1.4.8 1.3.7 1.4.8 1.4.7 1.3.8 1.4.8 1.3.7 1.4.8 1.3.7 1.3.8 1.3.7 1.2.8 1.3.8 1.2.7 1.2.8 1.2.7 1.2.8 1.2.7 1 .8 1.2.7 1 .8 1.1.8 1 .7 1 .8 1 .7 1 .8.9.7.9.8.8.8.9.7.8.8.8.7.8.8.7.7.7.8.7.8.7.7.6.8.6.7.6.8.6.7.5.8.5.7.5.8.5.8.5.7.4.8.4.7.4.8.4.7.4.8.3.8.4.7.3.8.3.7.2.8.3.7.3.8.2.8.2.7.2.8.2.7.2.8.2.7.2.8.1.7.2.8.1.8.2h.7l.8.2.7.1.8.1h.7l.8.2h.8l.7.1.8.1h.7l.8.1.7.1h.8l.8.1h.7l.8.1h1.5l.7.1h1.5l.8.1h3l.8.1h4.5l.8.1H300l.7.1h7.6"/></svg></center>

*I figuren: Standardnormalfördelningen.*

Observera att symmetri ger $\Phi(X)=1-\Phi(-X)$.

> **<u>I formelsamlingen:</u>**<br>
> **Tabell 1** används om gränsen $x$ är känd, och man söker $P(X\leq x)$.<br>
> **Tabell 2** används när $P(X\gt\lambda_\alpha)$ är känd, och gränsen ($\lambda_\alpha$) söks.
> 
> $\lambda_\alpha$ kallas för $\alpha$-kvartilen.

<!-- -->
> **Exempel:** Söker $P(\mu-2\sigma\lt X\lt\mu+2\sigma)$, givet $X\in\text N(\mu,\sigma)$.<br>
> Vi gör om till standardnormalfördelningen:<br>
> $P(\frac{\mu-2\sigma-\mu}\sigma\lt\frac{X-\mu}\sigma\lt\frac{\mu+2\sigma-\mu}\sigma)=P(-2\lt\frac{X-\mu}\sigma\lt2)=\left\{Y=\frac{X-\mu}\sigma\in N(0,1)\right\}=P(-2\lt Y\lt2)\\=P(Y\lt2)-P(Y\lt-2)=\Phi(2)-\Phi(-2)\\=\Phi(2)-(1-\Phi(2))=2\Phi(2)-1=0,954$

<!-- -->
> **Exempel:** Bestäm $k$ så att $P(\mu-k\sigma\lt X\lt\mu+k\sigma)=0,99$ om $X\in\text N(\mu,\sigma)$.<br>
> Gör om till $N(0,1)$:<br>
> $P(-k\lt\frac{X-\mu}\sigma\lt k)=0,99$
> 
> Anpassa till tabell 2:<br>
> $1-0.99=0.01=2P(\frac{X-\mu}\sigma\gt k)\\\implies0.005=P(\frac{X-\mu}\sigma\gt k)$<br>
> Tabell 2 ger då att $k=2,5758$.

**Sats:** *Varje linjärkombination av oberoende normalfördelade stokastiska variabler är normalfördelad.* (§ 4 i formelsamlingen)

> **Exempel 6.2:** $X\in\text N(28,0.25)$<br>
> a) Beräkna sannolikheten $P(27.5\lt X\lt 28.5)\\=P(\frac{27.5-28}{0.25}\lt\frac{X-28}{0.25}\lt\frac{28.5-28}{0.25})=P(-2\lt\frac{X-28}{0.25}\lt2)\\=0.9545$
> 
> b) Vi gör istället fyra mätningar och tar medelvärdet.<br>
> $\bar X=\frac14(X_1+X_2+X_3+X_4),\bar X\in\text N(28,\frac{0.25}{\sqrt4})$<br>
> Vi söker $P(27.5\lt\bar X\lt28.5)\\=P(\frac{27.5-28}{\frac{0.25}{\sqrt4}}\lt\frac{X-28}{\frac{0.25}{\sqrt4}}\lt\frac{28.5-28}{\frac{0.25}{\sqrt4}})\\=P(-4\lt\frac{X-28}{\frac{0.25}2}\lt4)$<br>
> $=1-2P(\frac{X-28}{\frac{0.25}2}\leq-4)\\=1-2(1-\Phi(4))=2\Phi(4)-1=2\cdot0,99997-1=0,99994$

### Centrala gränsvärdessatsen
> **Centrala gränsvärdessatsen:**<br>
> Summan $X_1,\dots,X_n$ av *många* oberoende och likafördelade (dvs. inte nödvändigtvis individuellt normalfördelade) stokastiska variabler med väntevärde $\mu$ och standardavvikelse $\sigma\gt0$ är *approximativt* normalfördelad $N(n\mu,\sigma\sqrt n)$.

Av samma anledning är även medelvärdet $\bar X$ av dessa s.v. approximativt normalfördelad enligt $N(\mu,\frac\sigma{\sqrt n})$. *Detta då medelvärdet är en linjärkombination av en enda, normalfördelad, stokastiskt variabel (se tidigare sats).*

**Hur många är *många*?**
Det handlar om symmetri. Om den ursprunliga fördelningen är hyfsat symmetrisk behövs inte särskilt många.

> **Exempel:** Avrundningsfel $X_i$ vid köp i affär. 48 kunder. Hur stor är sannolikheten att det totala avrundningsfelet är mindre än 6 kr?<br>
> $X_i\in U(-0.5,0,5)$<br>
> Låt $Y=X_1+\cdots+X_{48}$<br>
> Vi söker alltså $P(-6\lt Y\lt6)$<br>
> Många oberoende och likafördelade stokastiska variabler.<br>
> Enligt C.G.S. $Y\sim N(n\mu,\sigma\sqrt n),n=48,\mu=0,\sigma=\sqrt{\frac1{12}}$<br>
> $P(-6\lt Y\lt6)=P(\frac{-6}2\lt\frac Y2\lt\frac62)=P(-3\lt\frac Y2\lt3)\\=\Phi(3)-\Phi(-3)=2\Phi(3)-1=0.997$

## Föreläsning 8
*Anteckningar från Björn-Olof Skytt, föreläsning 8.*

Hypergeometriska fördelningar, binomialfördelningar och poissonfördelningar kan approximeras av normalfördelningen under vissa villkor.

> Repetition:<br>
> * **Hypergeometrisk fördelning:** *"Dragning utan återläggning"*<br>
> Antag att vi har $N$ stycken enheter där andelen med en egenskap är $p$, och att vi drar $n$ enheter utan återläggning.<br>
> $X="\text{antalet dragna med egenskapen}"$<br>
> Då är $X\in\text{Hyp}(N,n,p)$<br>
> $p_X(k)=\frac{{Np\choose k}{N(1-p)\choose n-k}}{N\choose n}$
> 
> * **Binomialfördelning:** *"Samma sannolikhet vid varje försök"*<br>
> Antag att vi genomför $n$ försök, och att vi varje gång lyckas med sannolikhet $p$.<br>
> $X="\text{antal lyckade försök}"$<br>
> Då är $X\in\text{Bin}(n,p)$<br>
> $p_X(k)={n\choose k}p^k(1-p)^{n-k}$

### Approximeringar
**När är $\text{Hyp}(N,n,p)\sim\text{Bin}(n,p)$?**<br>
Om det finns 10 gånger fler enheter från början, än antalet man drar, så är det okej att approximera. Dvs.

$$\frac Nn\geq10\implies\text{Hyp}(N,n,p)\sim\text{Bin}(n,p)$$

**När är $\text{Bin}(n,p)\sim\text{N}(np,\sqrt{np(1-p)}$**?<br>
Om $np(1-p)\geq10$ är det okej att approximera. Dvs.

$$np(1-p)\geq10\implies\text{Bin}(n,p)\sim\text{N}(np,\sqrt{np(1-p)})$$

**Förklaring:** Indikatorvariablerna är många och likafördelade. Enligt C.G.S. är då summan approx. normalfördelad. *Faktorn $p(1-p)$ är viktig och kan ses som ett symmetrivillkor (mer symmetrisk$\implies n$ kan vara mindre).*

> **Härledning av normalfördelnings-parametrar:**<br>
> Anta $X\in\text{Bin}(n,p)$ där $X="\text{antalet lyckade försök}"$<br>
> Vi skapar stokastiska *indikatorvariabler* $I_1,\dots,I_n$ som var och en antar värdet $1$ eller $0$, beroende på om försöket lyckas.<br>
> $\implies X=I_1+\cdots+I_n$
> 
> Väntevärde: $E(I_k)=\sum_{\text{alla }j}kp_I(j)=0(1-p)+1p=p$<br>
> Varians: $V(I_k)=E(I_k^2)-E^2(I_k)=p-p^2=p(1-p)$
> 
> Vi får:<br>
> $E(X)=E(\sum I_k)=np$<br>
> $V(X)=V(\sum I_k)=\{\text{ober.}\}=\sum V(I_k)=np(1-p)=\sigma^2$

### Halvkorrektion
> När vi approximerar en diskret fördelning till en kontinuerlig fördelning så går vi från att *summera* stolpar till att integrera en täthetsfunktion.

I diskreta fall är t.ex. $P(120\leq X\leq180)=P(119\lt X\lt181)$. Det uppstår inga problem (förutsatt att utfallen är heltal).<br>
I en kontinuerlig fördelning är det dock oviktigt huruvida gränserna ingår, och likheten ovan gäller då inte.

Vid **halvkorrektion** integrerar man över intervallet $[119.5,180.5]$.

> **Exempel:**<br>
> Den trendiga badbutiken Poolen o Plurret vill i slutet av säsongen bli av med 2000 badbyxor ur årets kollektion och anordnar därför en utförsäljning. Av erfarenhet vet man att antalet badbyxor som en kund köper dels är oberoende av hur många andra kunder köper, dels kan ses som en stokastisk variabel $X_i$ med sannolikhetsfunktion:
> 
> $$p_{X_i}(0)=0.2,\ p_{X_i}(1)=0.5,\ p_{X_i}(2)=0.2,\ p_{X_i}(3)=0.1$$
> 
> Beräkna med lämplig och välmotiverad approximation det minsta antalet kunder som måste komma till butiken om sannolikheten ska vara minst $90\%$ att butiken ska få samtliga badbyxor sålda.
> 
> $X_i$:na är många, oberoende och likafördelade.<br>
> $Y=X_1+\cdots+X_n$<br>
> Enligt *centrala gränsvärdessatsen*: $Y\sim\text N(n\mu,\sigma\sqrt n)$
> 
> Vi söker $n$ s.a. $P(Y\geq2000)\geq90\%$<br>
> $\mu=E(X_i)=0.5+2\cdot0.2+3\cdot0.1=1.2$<br>
> $V(X_i)=E(X_i^2)-E^2(X_i)=2.2-1.2^2=0.76=\sigma^2$<br>
> $\implies Y\sim\text N(1.2n,\sqrt{0.76n})$
> 
> Halvkorrektion ger $P(Y\geq1999.5)$, och efter approximation och justering till $\text N(0,1)$ får vi
> 
> $$P\left(\frac{Y-1.2n}{\sqrt{0.76n}}\geq\frac{1999.5-1.2n}{\sqrt{0.76n}}\right)=0.90\\\ \\\iff\frac{1999.5-1.2n}{\sqrt{0.76n}}=\{\text{ur formelsamlingen}\}=-1.2816$$
> 
> Sätt $\sqrt n=m$.<br>
> $m^2-\frac{1.2816}{1.2}\sqrt{0.76}m-\frac{1999.5}{1.2}=0$<br>
> $m=\{\text{positivt}\}=0.46553+40.82238$<br>
> $n=m^2=1704.69$<br>
> **Svar:** Det måste komma minst 1705 kunder.

### Approximation av poissonfördelningen
> **Poisson-fördelning:** En händelse inträffar med samma sannolikhet i varje punkt i ett intervall.<br>
> $X="\text{antal händelser som inträffar}"$<br>
> $X\in P_o(\mu)$

> **Sats:** Om $X\in P_o(\mu_1),Y\in P_o(\mu_2)$ och $X,Y$ oberoende, gäller att $X+Y\in P_o(\mu_1+\mu_2)$.

**När är $P_o(\mu)\sim\text{N}(\mu,\sqrt\mu)$?**<br>
Om $\mu\geq15$.
> **Motivering:**<br>
> Vi delar upp intervallet i $n$ st delintervall.<br>
> $X_i\in P_o(\mu_i)$<br>
> $Y=X_1+\cdots+X_n$
> 
> Om $X_i$:na är oberoende:<br>
> $\implies Y\in P_o(\mu_1+\cdots+\mu_n)=P_o(\mu)$
> 
> Även här har kan vi se villkoret som ett C.G.S.-villkor, ty om $n$ tillräckligt stort får vi ett $\mu\gt15$.

**När är $\text{Bin}(n,p)\sim P_o(np)$?**<br>
Om $p\leq0.1$.
> **Bevis:**<br>
> Antag att $X="\text{antal händelser som inträffar}"$<br>
> Vi delar upp tidsintervallet i $n$ st delintervall, där $n$ är stort.<br>
> Varje delintervall är så litet att det bara får plats högst en händelse där, och den inträffar med sannolikhet $p$.<br>
> Då blir totala antalet händelser $\text{Bin}(n,p)$.
> 
> Låt först $\mu=np$.<br>
> Vi börjar med binomialfördelningen:<br>
> $p_X(k)={n\choose k}p^k(1-p)^{n-k}\\=\frac{n!}{k!(n-k)!}\left(\frac\mu n\right)^k\left(1-\frac\mu n\right)^{n-k}$<br>
> Vi låter $p\to0$, och får då att $n\to\infty$. Detta ger:
> 
> $$\lim_{n\to\infty}\frac1{k!}\frac{n!}{n^k(n-k)!}\mu^k\left(1-\frac\mu n\right)^{n-k}\\=\left\{\frac{n!}{n^k(n-k)!}\to1\text{ ty }n\gg k\right\}\\=\lim_{n\to\infty}\frac{\mu^k}{k!}\left(1-\frac\mu n\right)^n\frac1{(1-\frac\mu n)^k}\\=\left\{\begin{gather*}(1-\frac\mu n)^n\to e^{-\mu}\\(1-\frac\mu n)^k\to1\end{gather*}\right\}=\frac{\mu^k}{k!}e^{-\mu}$$
> 
> Enligt formelsamlingen räcker det att $p\lt0.1$ för att $\text{Bin}(n,p)\sim P_o(np)$.

## Föreläsning 9
*Anteckningar från Björn-Olof Skytt, föreläsning 9.*<br>
*Boken: kap. 10, början av 11*

### Beskrivande statistik
|Sannolikhetslära|Statistik|
|-|-|
|Väntevärde $\mu$|Medelvärde $\bar x$|
|Varians $\sigma^2$|*Stickprovs*varians $s^2=\frac1{n-1}\sum_{j=1}^n(x_i-\bar x)^2$<br>*Populations*varians $\frac1n\sum_{i=1}^n(x_i-\bar x)^2$<br>Används vid stickprovs- resp. totalundersökning.|
|Variationskoefficient $R(X)=\frac{D(X)}{E(X)}$|Variationskoefficient $100\frac s{\bar x}\ \%$|
|Median $\tilde X$ så $F_X(\tilde X)=0.5$|Median $\tilde X=\text{"mittenvärdet"}$|
|Kovarians: $C(X,Y)\\=E((X-\mu_X)(Y-\mu_Y))$|Kovarians: $c_{xy}\\=\frac1{n-1}\sum(x_i-\bar x)(y_i-\bar y)$|
|Korrelationskoefficienten<br>$\rho(X,Y)=\frac{C(X,Y)}{D(X)D(Y)}$|Korrelationskoefficienten<br>$r_{xy}=\frac{c_{xy}}{s_xs_y}$|

#### Presentation av data
Anta att vi har följande data

|51|49|51|50|49|53|50|53|51|51|
|-|-|-|-|-|-|-|-|-|-|
|$x_1$|$x_2$|$x_3$|$x_4$|$x_5$|$x_6$|$x_7$|$x_8$|$x_9$|$x_10$|

**Grupperade data:** Vi räknar antal förekomster. T.ex. i tabell, stolpdiagram.

|Värde<br>$y_i$|Absolut frekvens<br>$f_i$|Relativ frekvens<br>$p_i$|
|-|-|-|
|49|2|20%|
|50|2|20%|
|51|4|40%|
|53|2|20%|

Medelvärde: $\bar x=\frac1n\sum_{i=1}^4f_iy_i$<br>
Stickprovsvarians: $s^2=\frac1{n-1}\sum_{i=1}^4f_i(y_i-\bar x)^2$

**Klassindelade data:**  Indelning av många data i klasser: 4 principer:

1. Vid beräkningar räknar vi som om alla data har klassmittens värde.
2. Inga öppna klasser (då saknas klassbredd)
3. Konstant klassbredd.

**Histogram:** Rektangelareor proportionella mot relativa frekvenser.

**Boxplot/lådagram:**<br>
<center><svg width="270" height="110"><path d="M59,19h152v64h-152v-64l2,2v60h148v-60h-148zm0,31h-45v2a2,2 0 1,0 -4,-2a2,2 0 1,0 4,2h45zm152,2h45v-2a2,2 0 1,0 4,2a2,2 0 1,0 -4,-2h-45zm-64,-31v60h2v-60z" fill="#000"/><text font-size="16" text-anchor="middle" font-family="KaTeX_Main, serif" font-style="italic"><tspan x="12" y="70">x&#x2098;&#x1D62;&#x2099;</tspan><tspan x="247" y="70">x&#x2098;&#x2090;&#x2093;</tspan><tspan x="135" y="100">x</tspan><tspan x="60" y="100">Q&#x2081;</tspan><tspan x="197" y="100">Q&#x2083;</tspan><tspan x="135" y="94">~</tspan></text></svg></center>

* Första (25%-) kvartilen: $Q_1$
* Andra (50%-) kvartilen (medianen): $Q_2$
* Tredje (75%-) kvartilen: $Q_3$
* 50% av mätdata ligger alltså inuti boxen.
* Kvartilintervall: $(Q_1,Q_3)$
* Kvartilavstånd: $Q_3-Q_1$
* Variationsintervall: $(X_\text{min},X_\text{max})$
* Variationsbredd: $X_\text{max}-X_\text{min}$

Om vi har $n$ st mätdata är $Q_1=x_k$ där $k$ är det tal som uppfyller

$$\frac 14n\leq k\leq\frac 14n+1$$

och på motsvarande sätt är $Q_3=x_k$ där $k$ är det tal som uppfyller

$$\frac34n\leq k\leq\frac34n+1$$

*Om två heltal uppfyller detta definieras kvartilen som medelvärdet av de 2 motsvarande värdena.*

### Punktskattning
Punktskattningen av en okänd storhet $\theta$ betecknas $\theta_\text{obs}^*$ och är ett utfall av den stokastiska variabeln $\theta^*$ (*stickprovsvariabeln*).<br>
Stickprovsvariabeln $\theta^*=\theta^*(X_1,\dots,X_n)$<br>
Punktskattningen $\theta_\text{obs}^*(x_1,\dots,x_n)$

> **Exempel:**<br>
> Vi vet att $E(X_i)=\mu$<br>
> $\mu_\text{obs}^*=\bar x$
> 
> **Exempel:**<br>
> Vi vet $E(X_i)=\mu$.<br>
> $\mu_\text{obs}^*=\frac{2x_1+9x_2}{11}$
> 
> **Exempel:**<br>
> Låt $D(X_i)=\sigma$.<br>
> Skattningen av std.avvikelsen: $\sigma_\text{obs}^*=s=\sqrt{\frac1{n-1}\sum(x_i-\bar x)^2}$<br>
> Detta är en stokastisk variabel, eftersom den beror på utfallen $x_1,\dots,x_n$ av de stokastiska variablerna $X_1,\dots,X_n$.<br>
> Denna stokastiska variabel kallas för *stickprovsvariabeln* och är:<br>
> $\sigma^*=S=\sqrt{\frac1{n-1}\sum(X_i-\bar X)^2}$

#### Skattningar: vanliga fördelningar
* **Bin-fördelning:**
$X\in\text{Bin}(n,p)$
Vi gör en skattning av sannolikheten: $p_\text{obs}^*=\frac xn$

* **Hypergeometrisk fördelning:** $X\in\text{Hyp}(N,n,p)$<br>
$p_\text{obs}^*=\frac xn$

* **Poisson-fördelning:** $X\in P_o(\mu)$<br>
Enligt formelsamlingen $\mu=E(X)$.<br>
Väntevärdet skattas likt tidigare $\mu_\text{obs}^*=\bar x$

* **För-första-gångenfördelning:** $X\in\text{ffg}(p)$<br>
Enligt formelsamlingen $E(X)=\frac1p$<br>
$\implies p_\text{obs}^*=\frac1{\bar x}$

Kontinuerliga fördelningar:

* **Exponentialfördelning:** $X\in\text{exp}(\lambda)$<br>
Enligt formelsamlingen $E(X)=\frac1\lambda$<br>
dvs. $\lambda_\text{obs}^*=\frac1{\bar x}$
* **Normalfördelning:** $X\in N(\mu,\sigma)$<br>
$\mu_\text{obs}^*=\bar x\\\sigma_\text{obs}^*=s$

> **Definition:**
> 
> $$\text{Skattningen }\theta_\text{obs}^*\text{ är konsistent}\\\iff\\\lim_{n\to\infty}P(|\theta_n^*-\theta|\gt\epsilon)\to0\\\text{där }\theta_n^*=\theta^*(X_1,\dots,X_n)$$

#### Maximum-likelihoodmetoden
Idén är att, eftersom man fått de mätdata man fått, så borde sannolikheten att få just dessa vara stor. Då maximerar vi denna sannolikhet m.a.p. den parameter vi vill skatta. Det värde på parametern som maximerar denna sannolikhet sätts till ML-skattningen.

Givet en observation $x_i$ av $X_i$, och att fördelningen på $X_i$ beror på en okänd parameter $\theta$, så kallas det värde $\theta_\text{obs}^*$ som maximerar likelihoodfunktionen

$$L(\theta)=\begin{cases}p_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta)&\text{diskreta fall}\\f_{X_1,\dots,X_n}(x_1,\dots,x_n;\theta)&\text{kontinuerliga fall}\end{cases}$$

för *maximum-likelihoodskattningen av $\theta$*.
> *Obs! $p_{X_i}(x_i;\theta)$ är bara ett sätt att förtydliga att sannolikhetsfunktionen beror av $\theta$; det förändrar i övrigt inte hur sannolikheten beräknas.*

<!-- -->
> **Exempel:** Anta att vi vet att $X_1,\dots,X_5$ är oberoende och att $X_i\in P_o(\mu)$. Vi får 5 mätdata: 10, 12, 7, 10, 4. Bestäm ML-skattningen av $\mu$.
> 
> $L(\mu)=P(X_1=10\cap X_2=12\cap\cdots\cap X_5=4)=\{\text{ober.}\}\\=p_{X_1}(10)p_{X_2}(12)\cdots p_{X_5}(4)=\frac{\mu^{10+12+7+10+4}}{10!\cdot12!\cdot7!\cdot10!\cdot4!}e^{-5\mu}$<br>
> > Vi konstaterar att $f(x_1)\gt f(x_2)\iff\ln(f(x_1))\gt\ln(f(x_2))$ för alla reella funktioner.
> 
> Genom att logaritmering får vi alltså att detta skall maximeras:<br>
> $43\ln{\mu}-5\mu-\ln(10!\cdot12!\cdot7!\cdot10!\cdot4!)$<br>
> Vi deriverar $\frac d{d\mu}\left(43\ln{\mu}-5\mu-\ln(10!\cdot12!\cdot7!\cdot10!\cdot4!)\right)\\=\frac{43}\mu-5=0$<br>
> Ett maximum är $\mu=\frac{43}5$<br>
> Detta $\mu$ maximerar likelihoodfunktionens värde, och kallas alltså *ML-skattningen av $\mu$*. Skattningen betecknas i vanlig ordning:
> 
> $$\mu_\text{obs}^*=\frac{43}5$$

## Föreläsning 10
*Anteckningar av Björn-Olof Skytts föreläsning 10.*<br>
*Boken: kap. 11*
> **Repetition:**<br>
> En *skattning* av det rätta värdet $\theta$ betecknas $\theta_\text{obs}^*$ ("den observerade skattningen av theta"), och är ett utfall av den stokastiska variabeln $\theta^*$.<br>
> Skattningen beror av våra mätresultat: $\theta_\text{obs}^*=\theta_\text{obs}^*(x_1,\dots,x_n)$<br>
> På motsvarande sätt: $\theta^*=\theta^*(X_1,\dots,X_n)$<br>
> T.ex. $\mu_\text{obs}^*=\bar x_n$ och $\mu^*=\bar X_n$.

### Minsta kvadratmetoden
Skattningen väljs så att summan av kvadratavstånden från verkliga mätdata minimeras.
> Enligt formelsamlingen §9:<br>
> $Q=\sum_{i=1}^n(x_i-\mu_i(\theta_1,\dots,\theta_n))^2$<br>
> där $\theta_1,\dots,\theta_n$ är de parametrar som vi vill skatta.<br>
> De $\theta_1,\dots,\theta_n$ som minimerar skillnaden mellan de uppmätta värdena $x_i$ och de teoretiska värdena $\mu_i$ väljs som MK-skattningen.

<!-- -->
> **Exempel:** Anta att vi vill skatta arean $\theta$ av en kvadrat med MKM, och att vi har mätt upp sidans längd och fått $x_1$ och $x_2$, och att diagonalens längd mätts till $x_3$.<br>
> Då gäller $E(X_1)=E(X_2)=\sqrt\theta$<br>
> $E(X_3)=\sqrt2\sqrt\theta$<br>
> Vi antar att variansen är $V(X_i)=\sigma^2$ för alla $i$.<br>
> $Q=\sum_{i=1}^3(x_i-\mu_i(\theta))^2\\=(x_1-\sqrt\theta)^2+(x_2-\sqrt\theta)^2+(x_3-\sqrt2\sqrt\theta)^2$<br>
> Vi vill nu välja $\theta$ så att denna summa minimeras.<br>
> Söker $0=\frac\partial{\partial\theta}=2(x_1-\sqrt\theta)(-\frac1{2\sqrt\theta})+2(x_2-\sqrt\theta)(-\frac1{2\sqrt\theta})+2(x_3-\sqrt2\sqrt\theta)(-\frac{\sqrt{2}}{2\sqrt\theta})$<br>
> $\iff x_1-\sqrt\theta+x_2-\sqrt\theta+\sqrt2x_3-2\sqrt\theta=0$<br>
> $\iff x_1+x_2+\sqrt2x_3=4\sqrt\theta$<br>
> **Svar:** Vi får att $\theta_{\text{obs}_\text{MK}}^*=\left(\frac{x_1+x_2+\sqrt2x_3}4\right)^2$

#### Skattning av flera variabler
Satsen i formelsamlingen tillåter skattningen av flera variabler.
> **Exempel 11.19:**<br>
> Vi mäter några vinklar och får:<br>
> AOB ($\theta_1$): Mätvärden $x_1,x_2,x_3$<br>
> BOC ($\theta_2$): Mätvärden $x_4,x_5$<br>
> AOC ($\theta_1+\theta_2$): Mätvärden $x_6,x_7$
> 
> Detta ger:<br>
> $Q=\sum_{i=1}^8(x_i-\mu_i(\theta_1,\theta_2))^2\\=(x_1-\theta_1)^2+(x_2-\theta_1)^2+(x_3-\theta_1)^2+(x_4-\theta_2)^2+(x_5-\theta_2)^2+(x_6-\theta_1-\theta_2)^2+(x_7-\theta_1-\theta_2)^2$
> 
> För att minimera detta söker vi $\text{grad}\ Q=(\frac{\partial Q}{\partial\theta_1},\frac{\partial Q}{\partial\theta_2})=(0,0)$.<br>
> Vi får $\begin{cases}x_1+x_2+x_3+x_6+x_7=5\theta_1+2\theta_2\\x_4+x_5+x_6+x_7=2\theta_1+4\theta_2\end{cases}$<br>
> Som vanligt betecknar vi lösningen med $\theta_{1_\text{obsMK}}^*,\theta_{2_\text{obsMK}}^*$

### Väntevärdesriktighet
$$\text{En skattning }\theta_\text{obs}^*\text{ av }\theta\text{ är väntevärdesriktig}\\\iff E(\theta^*)=\theta$$

Dvs. om vi gör många skattningar många gånger ska medelvärdet bli $\theta$.

> **Exempel:**<br>
> $\mu_\text{obs}^*=\frac{x_1+x_2+x_3}3$ om $E(X_i)=\mu$<br>
> $E(\mu^*)=E(\frac{X_1+X_2+X_3}3)=\frac13(E(X_1)+E(X_2)+E(X_3))=\mu$<br>
> Dvs. skattningen är *väntevärdesriktig*.
> 
> Om istället $\mu_\text{obs}^*=\frac{5x_3+3x_1+2x_2}{10}$ får vi:<br>
> $E(\mu^*)=E(\frac{5X_3+3X_1+2X_2}{10})=\frac1{10}(5\mu+3\mu+2\mu)=\mu$<br>
> Dvs. även denna skattning är *väntevärdesriktig*.

### Effektivitet
Om vi har flera väntevärdesriktiga skattningar av $\theta$ så är den skattningen som har lägst varians *den mest effektiva* skattningen.
> **Exempel:** Vi har två skattningar $\mu_\text{obs}^*,\hat\mu_\text{obs}^*$.<br>
> $\mu_\text{obs}^*=\frac{x_1+x_2+x_3}3$<br>
> $\hat{\mu}_\text{obs}^*=\frac{5x_3+3x_1+2x_2}{10}$
> 
> Vi antar att $V(X_i)=\sigma^2$ för alla $i$, och att $X_i$:na är oberoende.<br>
> $V(\mu^*)=V(\frac{X_1+X_2+X_3}3)=\frac193V(X_i)=\frac{V(X_i)}3=\frac{\sigma^2}3$<br>
> $V(\hat{\mu}^*)=V(\frac{5X_3+3X_1+2X_2}{10})=\frac1{100}(25+9+4)V(X_i)=\frac{38\sigma^2}{100}$<br>
> $V(\mu^*)\lt V(\hat\mu^*)\implies\mu_\text{obs}^*$ är en mer effektiv skattning

### Medelfel
En skattning av $D(\theta^*)$ kallas för *medelfelet* för $\theta^*$ och betecknas $d(\theta^*)$.<br>
Medelfelet för skattningen av $\theta$ är alltså $(D(\theta^*))_\text{obs}^*$.

> **Exempel:** Anta att $X\in N(\mu,\sigma)$. Vi har $\mu_\text{obs}^*=\bar x$.<br>
> Vad är medelfelet av denna skattning?<br>
> Vi vet att $\mu^*=\bar X=\frac1n\sum X$<br>
> $D(\mu^*)=D(\bar X)=\sqrt{V(\bar X)}=\sqrt{\frac{\sigma^2}n}=\frac\sigma{\sqrt{n}}$<br>
> Vi söker den observerade skattningen av detta, dvs.<br>
> $(D(\mu^*))_\text{obs}^*=\left(\frac\sigma{\sqrt n}\right)_\text{obs}^*=\frac{\sigma_\text{obs}^*}{\sqrt n}=\left\{\sigma_\text{obs}^*=\sqrt{s^2}\right\}=\frac s{\sqrt n}$

<!-- -->
> **Exempel:** $X\in\text{Bin}(n,p)$<br>
> $p_\text{obs}^*=\frac xn$<br>
> Vad är $d(p^*)$?<br>
> $d(p^*)=(D(p^*))_\text{obs}^*=\left(D(\frac Xn)\right)_\text{obs}^*=\left(\frac1n\sqrt{np(1-p)}\right)_\text{obs}^*\\=\left(\sqrt{\frac{p(1-p)}n}\right)_\text{obs}^*=\sqrt{\frac{p_\text{obs}^*(1-p_\text{obs}^*)}n}=\sqrt\frac{\frac xn(1-\frac xn)}{n}$

<!-- -->
> **Exempel:** $X\in P_o(\mu)$<br>
> Bestäm $d(\mu^*)$.<br>
> $\mu^*=\bar X$<br>
> $V(\bar X)=\frac1nV(X_i)=\{V(X_i)=\mu\}=\frac\mu n$<br>
> $d(\mu^*)=(D(\bar X))_\text{obs}^*=\left(\frac{\sqrt{\mu}}{\sqrt{n}}\right)_\text{obs}^*=\frac{\sqrt{\mu_\text{obs}^*}}{\sqrt n}=\sqrt{\frac{\bar x}{n}}$

### Konfidensintervall
Ett intervall $I_\theta$ som med sannolikhet $1-\alpha$ täcker över $\theta$ kallas för ett *konfidensintervall* för $\theta$ med konfidensgrad $1-\alpha$.

Dvs. $P(\theta\in I_\theta)=1-\alpha$

> **Exempel:** Anta att vi vill skatta $\mu$ med $\bar x$ och att vi vill bilda ett konfidensintervall så att vi med 95%-ig sannolikhet täcker över $\mu$.<br>
> $I_\mu=[\bar x-a,\bar x+a]$<br>
> $P(\bar X-a\lt\mu\lt\bar X+a)=0.95$<br>
> Observera att det är $\bar x$ som är slumpmässig!<br>
> Varje gång vi slumpar $\bar x$ får vi ett konfidensintervall.<br>
> Om vi slumpar $\bar x$ 100 gånger så kommer konfidensintervallen i genomsnitt att tecka över $\mu$ (det verkliga värdet) 95 ggr.

<!-- -->
> **Exempel:** Anta nu att $X_i$:na är oberoende och $X_i\in N(\mu,\sigma)$.<br>
> Vi gör $n$ st mätningar och vill bilda ett konfidensintervall av graden $1-\alpha$ för $\mu$.
> 
> Vi skattar först $\mu$ med $\bar x$. Notera att $\bar X\in N(\mu,\frac\sigma{\sqrt n})$.<br>
> Vi söker $a$ så att $P(\mu-a\leq\bar X\leq\mu+a)=1-\alpha$.<br>
> $\iff P(\frac{\mu-a-\mu}{\frac\sigma{\sqrt n}}\leq\frac{\bar X-\mu}{\frac\sigma{\sqrt n}}\leq\frac{\mu+a-\mu}{\frac\sigma{\sqrt n}})=1-\alpha$<br>
> $\iff2\Phi(\frac a{\frac\sigma{\sqrt n}})-1=1-\alpha$<br>
> $\iff\{Y\in N(0,1)\}\iff P(Y\gt\frac a{\frac\sigma{\sqrt n}})=\frac\alpha2$<br>
> $\implies I_\mu=\bar x\pm\frac\sigma{\sqrt n}\lambda_{\alpha/2}$

#### Användning av §12
Vi ska bilda konfidensintervall för väntevärdet när $\sigma$ är känt.<br>
Enligt §12.1 gäller att $I_\theta=\theta_\text{obs}^*\pm\lambda_{\alpha/2}$.<br>
Vi har $\theta=\mu$ och alltså $\theta_\text{obs}^*=\mu_\text{obs}^*=\bar x$.<br>
Enligt §12.1 gäller $\theta^*\in N(\theta,D)$.<br>
$D=D(\theta^*)=D(\mu^*)=D(\bar X)=\frac\sigma{\sqrt n}$<br>
Slutligen har vi:<br>
$I_\theta=\bar x+\frac\sigma{\sqrt n}\lambda_{\alpha/2}$

Konfidensintervallets bredd beror alltså på spridningen $\sigma$, antalet observationer $n$ och önskad säkerhet $1-\alpha$.

## Föreläsning 11
*Videoföreläsning #11 av Björn-Olof Skytt.*

### Mer om konfidensintervall
> **Repetition:** Vi har oberoende $X_i\in N(\mu,\sigma)$.<br>
> Vi vill bilda ett konfidensintervall för $\mu$ med konfidensgrad $1-\alpha$.<br>
> Vi söker $a$ så att $P(|\mu^*-\mu|\leq a)=1-\alpha$<br>
> $\iff P(-\frac a{\sigma/\sqrt n}\leq\frac{\bar X-\mu}{\sigma/\sqrt n}\leq\frac a{\sigma/\sqrt n})\\\iff\Phi(\frac a{\sigma/\sqrt n})=1-\frac\alpha2\\I_\mu=[\bar x-\frac\sigma{\sqrt n}\lambda_{\alpha/2},\bar x+\frac\sigma{\sqrt n}\lambda_{\alpha/2}]$

Anta nu att vi har samma problem, *men att $\sigma$ är okänt*. Vi är därför tvungna att skatta $\sigma$, dvs. $\sigma_\text{obs}^*=s=\sqrt{\frac1{n-1}\sum(x_i-\bar x)^2}$. Vi söker återigen $a$ s.a. $P(-\frac a{S/\sqrt n}\leq\frac{\bar X-\mu}{S/\sqrt n}\leq\frac a{S/\sqrt n})$.

Observera att $\frac{\bar X-\mu}{S/\sqrt n}\notin N(1,0)$ i detta fall, eftersom nämnaren inte är en konstant utan också innehåller en stokastisk variabel $S$. Istället uppstår en *t-fördelning* (§11.1d):

$$\frac{\bar X-\mu}{S/\sqrt n}\in t(n-1)$$

Vi kan ta fram konfidensintervallet för $\mu$ då $\sigma$ är okänt med hjälp av §12.2:

$$I_\theta=\theta_\text{obs}^*\pm D_\text{obs}^*t_{\alpha/2}(f)$$

där $\frac{\theta^*-\theta}{D^*}\in t(f)$. Enligt §11.1d är $\frac{\bar X-\mu}{S/\sqrt n}\in t(n-1)$, vilket ger oss att $D^*=S/\sqrt n\implies D_\text{obs}^*=s/\sqrt n$, och $f=n-1$. Vi får slutligen:

$$I_\mu=\bar x\pm\frac s{\sqrt n}t_{\alpha/2}(n-1)$$

*Tab. 3 visar $P(X\gt t_\alpha(f))=\alpha$, där $f$ är antalet frihetsgrader (överkurs).*

> **Exempel:** Låt $a-\alpha=0.95$ och att vi har 15 mätdata.<br>
> $\implies t_{\alpha/2}(n-1)=t_{0.025}(14)=2.14$<br>
> Om $\sigma$ istället är känd hade vi haft $\lambda_{\alpha/2}=1.96$.<br>
> Intervallet blir alltså bredare om $\sigma$ är okänt.<br>
> Om vi gör oändligt många försök får vi att $t_{\alpha/2}(\infty-1)=1.96$, eftersom *det medför att standardavvikelsen $\sigma$ är känd*.

#### Skillnad mellan två stickprov
Vi antar att vi har oberoende $X_i\in N(\mu_x,\sigma_x)$, och även oberoende $Y_i\in N(\mu_y,\sigma_y)$. Vi har 3 möjliga fall:

1) $\sigma_x,\sigma_y$ är kända<br>
Vi får likt tidigare $P(-\lambda_{\alpha/2}\leq\frac{\bar X-\bar Y-(\mu_x-\mu_y)}{D(\bar X-\bar Y)}\leq\lambda_{\alpha/2})=1-\alpha$<br>
där $D(\bar X-\bar Y)=\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}$. Detta ger konfidensintervallet

$$I_{\mu_x-\mu_y}=\bar X-\bar Y\pm D(\bar X-\bar Y)\lambda_{\alpha/2}$$

2) $\sigma_x,\sigma_y$ är *okända och olika*<br>
Enligt §12.3 är $I_\theta=\theta_\text{obs}^*\pm D_\text{obs}^*\lambda_{\alpha/2}$. I vårt fall får vi

$$I_{\mu_x-\mu_y}=\bar x-\bar y\pm\sqrt{\frac{s_x^2}{n_x}+\frac{s_y^2}{n_y}}\lambda_{\alpha/2}$$

med **approximativ konfidensgrad $1-\alpha$**.

3) $\sigma_x=\sigma_y=\sigma$ är *okänd*<br>
$P(-t_{\alpha/2}(f)\lt\frac{\bar X-\bar Y-(\mu_x-\mu_y)}{\sqrt{\frac{s_x^2}{n_x}+\frac{s_y^2}{n_y}}}\lt t_{\alpha/2}(f))=1-\alpha$<br>
Hur skattar vi $\sigma$ nu när vi har två stickprov? Vi viktar:

$$s^2=\frac{(n_x-1)s_x^2+(n_y-1)s_y^2}{n_x+n_y-2}$$

Vi tar sedan fram $I_{\mu_x-\mu_y}$ med § 12.2:<br>
$I_\theta=\theta_\text{obs}^*\pm D_\text{obs}^*t_{\alpha/2}(f)$<br>
där $\theta_\text{obs}^*=\bar x-\bar y$. Detta ger oss<br>
$I_{\mu_x-\mu_y}=\bar x-\bar y\pm s\sqrt{\frac1{n_x}+\frac1{n_y}}t_{\alpha/2}(n_x+n_y-2)$

#### Stickprov i par
Anta att vi har parvisa observationer av $X_i\in N(\mu_x,\sigma_x)$ och $Y_i\in N(\mu_x+\Delta,\sigma_y)$, och att vi vill ställa upp ett konfidensintervall för $\Delta$.

Vi gör om detta till ett enda stickprov genom att bilda $z_i=y_i-x_i$. Då har vi att $Z_i\in N(\mu_z,\sigma_z)=N(\Delta,\sigma_z)$.
$\implies I_\mu=\bar z\pm\frac{s_Z}{\sqrt n}t_{\alpha/2}(n-1)$


> **Exempel:** (tenta 2019/06, uppg. 15)
> 
> |Härdningstid|Hållfasthet|
> |-|-|
> |2 dagar|21.8, 21.7, ..., 21.9 (7 datapunkter)|
> |7 dagar|32.4, 31.8, ..., 34.9 (7 datapunkter)|
> 
> Vi vill undersöka om det är någon skillnad mellan att $X$ härda 2 dagar och $Y$ att härda 7 dagar. $X,Y$ normalfördelade med samma standardavvikelse $\sigma$. Alla stokastiska variabler är oberoende.<br>
> Vi har alltså en situation med skillnad mellan 2 stickprov när standardavvikelserna antas vara *lika men okända*.<br>
> Låt $\theta=\mu_Y-\mu_X$<br>
> $\theta_\text{obs}^*=\bar y-\bar x$<br>
> $D_\text{obs}^*=s\sqrt{\frac1{n_x}+\frac1{n_y}}$<br>
> där $s^2=\frac{(n_x-1)s_x^2+(n_y-1)s_y^2}{n_x+n_y-2}$<br>
> $I_{\mu_Y-\mu_X}=\bar y-\bar x\pm s\sqrt{\frac17+\frac17}t_{0.05}(12)=12.10\pm0.95$<br>

<!-- -->
> **Exempel:** (tenta 2019/08, uppg. 15)<br>
> För att undersöka om en förpackningsmaskin är korrekt inställd görs följande försök: 5 enheter tar ut. Dessa vägs först med innehåll (i), och sedan utan innehåll (ii):
> 
> |Enhet#|1|2|3|4|5|
> |-|-|-|-|-|-|
> |(i)|114|124|115|117|123|
> |(ii)|18|23|19|20|24|
> 
> Vi vill ta fram ett konfidensintervall $\alpha=0.05$ för vikten av förpackningarna.<br>
> $X_k\in N(\mu_k,\sigma_1)$ - Vikt hos tom förpackning<br>
> $Y_k\in N(\mu_k+\Delta,\sigma_2)$ - Vikt hos fylld förpackning<br>
> Detta är en situation med stickprov i par.<br>
> Vi bildar $Z_k=Y_k-X_k\in N(\Delta,\sigma_Z)$.<br>
> Vi har nu stickprov $z_1,\dots=96,101,96,97,99$<br>
> Vi vill bilda konfidensintervall för väntevärdet när $\sigma$ är okänd.<br>
> $D_\text{obs}^*=s/\sqrt n=\frac{\sqrt{47}}{\sqrt5}$<br>
> $I_\Delta=\Delta_\text{obs}^*\pm D_\text{obs}^*t_{0.025}(n-1)\\=\bar z\pm\sqrt{47/5}t_{0.025}(4)\approx97.8\pm2.7$

## Föreläsning 12
*Videoföreläsning #12 av Björn-Olof Skytt.*

### Konfidensintervall med approximativ konfidensgrad
När vi inte har normalfördelning från början, men vi bildar ett konfidensintervall med hjälp av en normalapproximation.
> Om $\theta^*\sim N$ ges konfidensintervallet med *approximativ konfidensgrad* $1-\alpha$ av $I_\theta=\theta_\text{obs}^*\pm d(\theta^*)\lambda_{\alpha/2}$

<!-- -->
> **Exempel:** (med CGS)<br>
> Anta att vi har två stickprov $x_1,\dots,x_n$ och $y_1,\dots,y_m$, där dock $X_i,Y_i$ inte är normalfördelade.<br>
> $E(X_i)=\mu_X,E(Y_j)=\mu_Y\\D(X_i)=\sigma_X,D(Y_j)=\sigma_Y$<br>
> där $\mu_X,\mu_Y$ är okända.<br>
> Vi vill du bilda konfidensintervall för $\mu_X-\mu_Y$. Vi använder §12.3:<br>
> $I_\theta=\theta_\text{obs}^*\pm D_\text{obs}^*\lambda_{\alpha/2}$<br>
> Villkoret för detta är enligt §12.3 att $\theta\sim N$.<br>
> $\theta^*=(\mu_X-\mu_Y)^*=\bar X-\bar Y=\frac1n\sum X_i-\frac1m\sum Y_j$<br>
> Eftersom $X_i$:na och $Y_j$:na är många, oberoende och likafördelade, är $\bar X,\bar Y$ approx. normalfördelade, och därmed ochså $\bar X-\bar Y=\theta^*$ det. Vi får alltså använda §12.3:<br>
> $D=D(\theta^*)=\sqrt{V(\bar X-\bar Y)}=\sqrt{V(\bar X)+V(\bar Y)}\\=\sqrt{\frac1n\sigma_X^2+\frac1m\sigma_Y^2}$<br>
> $\implies D_\text{obs}^*=\sqrt{\frac1ns_X^2+\frac1ms_Y^2}$<br>
> **Svar:** $I_{\mu_X-\mu_Y}=\bar x-\bar y\pm \sqrt{\frac1ns_X^2+\frac1ms_Y^2}\lambda_{\alpha/2}$

<!-- -->
> **Exempel:** (via approx. av binomialfördelning)<br>
> Vi har $X\in\text{Bin}(n,p)$ och vill skatta $p$.<br>
> $p^*=\frac Xn\sim\{\text{om }X\sim N\}\sim N$<br>
> För att kunna approximera $X\sim N$ ska $np(1-p)\geq10$.<br>
> $p$ är dock okänd, och vi får därför $np_\text{obs}^*(1-p_\text{obs}^*)\geq10$.<br>
> $I_p=p_\text{obs}^*\pm D_\text{obs}^*\lambda_{\alpha/2}$<br>
> $D=D(p^*)=D(\frac Xn)=\frac1nD(X)=\frac1n\sqrt{np(1-p)}$<br>
> $D_\text{obs}^*=\frac1n\sqrt{np_\text{obs}^*(1-p_\text{obs}^*)}=\sqrt{\frac{\frac xn(1-\frac xn)}n}$<br>
> Vi får:
> 
> $$I_p=\frac xn\pm\sqrt{\frac{\frac xn(1-\frac xn)}n}\lambda_{\alpha/2}$$

<!-- -->
> **Exempel:** Opinionsundersökning<br>
> Vi vill ta reda på konfidensintervall för skillnad mellan andelar.<br>
> Vi har $X\in\text{Bin}(n_X,p_X)$ och $Y\in\text{Bin}(n_Y,p_Y)$. Anta $X,Y$ oberoende.<br>
> Vi vill bilda konfidensintervall för $p_X-p_Y$.<br>
> $I_\theta=\theta_\text{obs}^*\pm D_\text{obs}^*\lambda_{\alpha/2}$<br>
> $\theta=p_X-p_Y$<br>
> $\theta_\text{obs}^*={p_X}_\text{obs}^*-{p_Y}_\text{obs}^*=\frac x{n_X}-\frac y{n_Y}$<br>
> $D=D(\theta^*)=\sqrt{V\left(\frac X{n_X}-\frac Y{n_Y}\right)}=\sqrt{\frac1{n_X^2}V(X)+\frac1{n_Y^2}V(Y)}\\=\sqrt{\frac1{n_X^2}n_Xp_X(1-p_X)+\frac1{n_Y^2}n_Yp_Y(1-p_Y)}$<br>
> Vi får:<br>
> $I_{p_X-p_Y}=\frac x{n_X}-\frac y{n_Y}\pm\sqrt{\frac{{p_X}_\text{obs}^*\cdot(1-{p_X}_\text{obs}^*)}{n_X}+\frac{{p_Y}_\text{obs}^*\cdot(1-{p_Y}_\text{obs}^*)}{n_Y}}\lambda_{\alpha/2}$

<!-- -->
> **Exempel:** (poissonfördelning)<br>
> Vi antar $X_i\in P_o(\mu)$. Enligt §3 är $\mu=E(X)$.<br>
> Vi vill skatta $\mu$ och vill bilda ett konfidensintervall.<br>
> Eftersom vi inte har normalfördelning vill vi använda §12.3.<br>
> $\theta^*=\mu^*=\bar X\sim\{\text{om }\bar X\sim N\}\sim N$<br>
> $\theta_\text{obs}^*=\bar x$<br>
> Vi vet att $X\in P_o(\mu)$.<br>
> $\implies\sum X_i\in P_o(n\bar x)\sim N(n\bar x,\sqrt{n\bar x})$ om $n\bar x\geq15$<br>
> $D=D(\theta^*)=D(\bar X)=\frac1n\sqrt{\sum V(X_i)}=\frac1n\sqrt{n\mu}=\sqrt{\frac{\mu}n}$<br>
> $D_\text{obs}^*=\sqrt{\frac{\bar x}n}$<br>
> Vi får alltså:<br>
> $I_\mu=\bar x\pm\sqrt{\frac{\bar x}n}\lambda_{\alpha/2}$<br>
> **Obs!** I detta fall använder vi inte stickprovsvariansen $s^2$ eftersom vi känner till att $X_i$ är poissonfördelade (variansen ges då enl. §3).

### Konfidensintervall för $\sigma$ och $\sigma^2$
**Bakgrund:** Om vi har $X_1,\dots,X_f\in N(0,1)$ och oberoende så är $\sum X_i^2\in\chi^2(f)$.

Anta att $X_i$:na oberoende och $N(\mu,\sigma)$.<br>
Då gäller att<br>
$\sum(\frac{X_i-\bar x}\sigma)^2=\frac1{\sigma^2}\sum(X_i-\bar x)^2=\frac{n-1}{\sigma^2}\frac1{n-1}\sum(X_i-\bar x)^2\\=\frac{n-1}{\sigma^2}s^2\in\chi^2(n-1)$

Vi använder tabell 4 och får<br>
$P(\frac{n-1}{\sigma^2}s^2\gt\chi^2_\alpha(n-1))=\alpha$<br>
$\implies P(\sigma\lt\sqrt{\frac{(n-1)s^2}{\chi^2_\alpha(n-1)}})=\alpha$

Nedre gränsen: $P(\sigma\lt\sqrt{\frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)}})=\frac\alpha2$<br>
Övre gränsen: $P(\sigma\lt\sqrt{\frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}})=1-\frac\alpha2$<br>
$\implies I_\sigma=\left(\sqrt{\frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)}},\sqrt{\frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}}\right)$<br>
*Observera att konfidensintervallet inte är symmetriskt då $\chi^2$-fördelningen inte är det!*

#### Hur man hittar konfidensintervall med formelsamlingen
Använd §12.4:<br>
$I_\theta=\left(\theta_\text{obs}^*\sqrt{\frac f{\chi^2_{\alpha/2}(f)}},\theta_\text{obs}^*\sqrt{\frac f{\chi^2_{1-\alpha/2}(f)}}\right)$
Här ställs kravet att $f\cdot(\frac{\theta^*}\theta)^2\in\chi^2(f)$. Detta kan jämföras med §11.1b som säger att $(n-1)\frac{S^2}{\sigma^2}\in\chi^2(n-1)$.

Vi ser att $f=n-1,\theta=\sigma,\theta^*=S$ uppfyller detta:<br>
$I_\sigma=\left(s\sqrt{\frac{n-1}{\chi^2_{\alpha/2}(n-1)}},s\sqrt{\frac{n-1}{\chi^2_{1-\alpha/2}(n-1)}}\right)$<br>
Genom att kvadrera gränserna får vi enkelt:<br>
$I_{\sigma^2}=\left(\frac{s^2(n-1)}{\chi^2_{\alpha/2}(n-1)},\frac{s^2(n-1)}{\chi^2_{1-\alpha/2}(n-1)}\right)$

### Felfortplantning
Se kapitel 11.10, s. 273-.
> **Uppgift 11.13b:**<br>
> Vi har skattningen $\lambda_\text{obs}^*=\frac1{\bar x}$ där vi har oberoende $X_i\in\text{exp}(\lambda)$.<br>
> Vi söker nu väntevärdet $E(\lambda^*)$ och medelfelet $\left(D(\lambda^*)\right)_\text{obs}^*$.
> 
> *Räknereglerna för väntevärde och varians förutsätter att vi har linjärkombinationer av stokastiska variabler. Här har vi dock* $\frac1{\bar X}=\frac n{\sum X_i}$.
> 
> $D(\lambda^*)=D(\frac1{\bar X})\approx\{\text{se §9.4}\}\approx|g'(\bar X)|\cdot D(\bar X)\\=\frac1{\left(\bar X\right)^2}\cdot\frac1{\sqrt n}D(X_i)=\frac1{\left(\bar X\right)^2}\cdot\frac1{\lambda\sqrt n}$<br>
> Medelfelet blir därmed:<br>
> $d(\lambda^*)\approx\frac1{\left(\bar x\right)^2}\cdot\frac1{\lambda_\text{obs}^*\sqrt n}=\frac1{\left(\bar x\right)^2}\cdot\frac1{\frac1{\bar x}\sqrt n}=\frac1{\bar x\sqrt n}$

<!-- -->
> **Härledning av §9.4:**<br>
> Vi antar att $\theta_\text{obs}^*$ är en väntevärdesriktig skattning av $\theta$.<br>
> Vi använder Taylor-utveckling:<br>
> $g(\theta^*)=g(\theta)+g'(\theta)(\theta^*-\theta)+\text{felterm}$<br>
> Vi får då:<br>
> $E(g(\theta^*))\approx E(g(\theta)+g'(\theta)(\theta^*-\theta))=g(\theta)+g'(\theta)(E(\theta^*)-\theta)=\{\text{väntevärdesriktig, dvs. }E(\theta^*)=\theta\}\\=g(\theta)=\{\text{väntevärdesriktighet}\}=g(E(\theta^*))$<br>
> 
> Motsvarande varians:<br>
> $V(g(\theta^*))\approx V(g(\theta)+g'(\theta)(\theta^*-\theta))\\=(g'(\theta))^2\cdot V(\theta^*-\theta)=(g'(\theta))^2\cdot V(\theta^*)$<br>
> $\implies D(g(\theta^*))\approx |g'(\theta)|\cdot D(\theta^*)$<br>
> Medelfelet:<br>
> $d(g(\theta^*))=(D(g(\theta^*)))_\text{obs}^*\approx|g'(\theta_\text{obs}^*)|\cdot (D(\theta^*))_\text{obs}^*$

## Föreläsning 13
*Videoföreläsning #13 av Björn-Olof Skytt.*

### Hypotesprövning
Den hypotes man vill testa om man ska förkasta eller inte kallas för *nollhypotesen* $H_0$. En *nollhypotes* måste alltid kunna förkastas. Mothypotesen kallas för $H_1$.

**Felrisk = signifikansnivå = $\alpha$**<br>
Den *högsta* risken vi är villiga att ta att förkasta $H_0$ om $H_0$ är sann. Dvs. om $P(\text{förkasta }H_0|H_0\text{ är sann})\lt\alpha$ så förkastar vi $H_0$ med felrisken $\alpha$ (annars inte).

**Testvariabel:**<br>
För att pröva $H_0$ hittar vi på en *testvariabel* $t_\text{obs}=t(x)$. Testvariabeln är en observation av stickprovsvariabeln $t(X)$.

**Kritiskt område:**<br>
Det *kritiska området* är en del av det område som $t_\text{obs}$ varierar över. För att uppfylla kravet på felrisk ska det gälla att:

$$P(t(X)\in C|H_0\text{ sann})=\alpha$$

*Signifikanstest:* Om $t_\text{obs}$ faller inom det kritiska området förkastas $H_0$. Resultatet sägs då vara *signifikant* på nivån $\alpha$.

$$\begin{matrix}t_\text{obs}\in C&\implies&\text{förkasta }H_0\\t_\text{obs}\notin C&\implies&\text{förkasta inte }H_0\end{matrix}$$

***OBS!*** Icke-signifikant resultat (*förkasta inte*)$\rlap{\(\quad\not\)}\implies H_0$ sant

**p-värdet (observerad signifikansnivå)**<br>
$P(\text{förkasta }H_0|H_0\text{ är sann \& visst värde på testvariabeln})$

Om p-värdet $<\alpha\implies$Förkasta $H_0$ (annars inte)

**Styrkefunktionen:** $h(\theta)=\dots$ (se §14.1 i formelsamlingen)<br>
$h(\theta)=P(\text{förkasta }H_0|\theta\text{ är rätt parametervärde})$

För $\theta\in H_0$ bör $h(\theta)$ vara litet.
Om t.ex. $H_0$ är  att $\theta=\theta_0$ gäller att $h(\theta_0)=\alpha$.

**Styrkan hos test för $\theta=\theta_1$**:<br>
$=P(\text{förkasta }H_0:\theta=\theta_0)$ om $H_1:\theta=\theta_1$ är sann.

### Metoder för hypotesprövning
Vi har, grovt sett, två sett att genomföra hypotesprövning.

#### p-värdesmetoden
> **Exempel 13.1:**<br>
> Vi har en person som påstår sig kunna förutse krona/klave.<br>
> Vi testar detta genom att singla slanten en massa gånger med vetskapen att $P(\text{gissa rätt})=\frac12$. Om $X="\text{antal rätt}"$ gäller att $X\in\text{Bin}(n,p)$<br>
> $H_0:p=\frac12$ (dvs. att personen gissar)<br>
> Vi låter personen göra 12 försök.<br>
> Under $H_0$ gäller att $X\in\text{Bin}(12,\frac12)$<br>
> $\implies P(X\geq10)=0.01929,P(X\geq9)=0.073$<br>
> Vi väljer nu ett **kritiskt område**, dvs. det område där $H_0$ förkastas.<br>
> Anta att vi valt risknivån $\alpha=0.05$.<br>
> Eftersom vi inte får överskrida risknivån väljer vi $x\geq10$ som kriterium för att förkasta $H_0$.<br>
> Dvs. vårt *kritiska område* $C=\{x|x\geq10\}$.<br>
> Testvariabeln är vår observation.<br>
> Anta att $x=10$. p-värdet blir då:<br>
> $P(X\geq10)=0.01929\lt\alpha=0.05\implies$ förkasta $H_0$<br>
> Anta att vi fått $x=9$

> **Exempel 13.4:** Säg att personen svarar rätt på 9/10.<br>
> Vi låter nollhypotesen vara $H_0:p=\frac12$,<br>
> och en mothypotes $H_1:p=\frac9{10}$.<br>
> Låt $\alpha=0.05$, vilket likt tidigare innebär att $H_0$ kan förkastas om $x\geq10$. Vi får styrkan hos det tidigare formulerade **testet**:<br>
> $h(0.9)=P(\text{förkasta }H_0|p=0.9)=P(X\geq10|p=0.9)\\=\sum_{i=10}^{12}{12\choose i}0.9^i0.1^{12-i}=0.89$

#### Med konfidensintervall
Vi förkastar om utanför konfidensintervallet.
> **Exempel 13.8:** Anta oberoende $X_i\in N(\mu,\sigma)$.<br>
> $H_0:\mu=17.0$ och $H_1:\mu\neq17.0$<br>
> Vi gör 60 mätningar och får $\bar x=16.51$ och $\frac s{\sqrt n}=0.159$.<br>
> Vi bildar ett tvåsidigt konfidensintervall för väntevärdet för $\mu$ när $\sigma$ okänd med hjälp av §12.2.
> 
> $I_\mu=\bar x\pm\frac s{\sqrt n}t_{\alpha/2}(n-1)$<br>
> Om risknivån är $\alpha=0.01$ blir $t_{\alpha/2}(n-1)=t_{0.005}(59)$. Vi får konfidensintervallet $16.51\pm0.42$ (se figur).
> 
> <center><svg width="400" viewBox="0 0 600 350"><defs><pattern id="d" patternUnits="userSpaceOnUse" width="4" height="4"><path d="M-1,1l2,-2M0,4 l4,-4M3,5 l2,-2" stroke="pink"/></pattern></defs><path d="M45,300 531,300M45,250 531,250M45,200 531,200M45,150 531,150M45,100 531,100M45,50 531,50" stroke="grey" fill="none"/><path d="M539.4,295.8L552,300L539.4,304.2L541.5,300z" stroke="black"/><path d="M34,300 541,300z" stroke-width="2" stroke="black" fill="none"/><text font-family="KaTeX_Main, serif" font-size="25"><tspan x="55" y="258">0.5</tspan><tspan x="55" y="208">1</tspan><tspan x="55" y="158">1.5</tspan><tspan x="55" y="108">2</tspan><tspan x="55" y="58">2.5</tspan><tspan x="562" y="308" font-style="italic">x</tspan><tspan x="290" y="328" text-anchor="middle">16.51</tspan><tspan x="199" y="328" text-anchor="middle" fill="green">16.09</tspan><tspan x="380" y="328" text-anchor="middle" fill="green">16.93</tspan></text><path d="M30,300 182.7,300 183.5,299.9 190,299.9 190.8,299.8 193.2,299.8 194,299.7 194.8,299.7 195.7,299.7 196.5,299.6 197.3,299.6 198,299.5 198.9,299.5 199.7,299.4 200.5,299.4 201.3,299.3 202.1,299.2 203,299.1 203.8,299 204.6,298.9 205.4,298.8 206.2,298.7 207,298.5 207.8,298.4 208.6,298.2 209.4,298 210.2,297.8 211,297.6 211.9,297.4 212.7,297.1 213.5,296.8 214.3,296.5 215.1,296.2 216,295.8 216.7,295.4 217.5,295 218.3,294.5 219.1,294 220,293.5 220.8,293 221.6,292.3 222.4,291.7 223.2,291 224,290.2 224.8,289.4 225.6,288.5 226.4,287.6 227.2,286.6 228,285.6 228.9,284.5 229.7,283.3 230.5,282 231.3,280.7 232.1,279.2 233,277.8 233.7,276.2 234.5,274.5 235.3,272.7 236.2,270.9 237,268.9 237.8,266.9 238.6,264.7 239.4,262.5 240.2,260.1 241,257.7 241.8,255.1 242.6,252.5 243.4,249.7 244.3,246.8 245,243.8 245.9,240.7 246.7,237.4 247.5,234.1 248.3,230.7 249.1,227.1 250,223.5 250.7,219.7 251.5,215.8 252.4,211.9 253.2,207.8 254,203.7 254.8,199.4 255.6,195.1 256.4,190.7 257.2,186.3 258,181.7 258.8,177.1 259.6,172.5 260.5,167.8 261.3,163.1 262,158.3 262.9,153.6 263.7,148.8 264.5,144 265.3,139.2 266.1,134.5 266.9,129.7 267.7,125 268.6,120.4 269.4,115.8 270.2,111.3 271,106.8 271.8,102.5 272.6,98.3 273.4,94.1 274.2,90.1 275,86.2 275.8,82.5 276.7,78.9 277.5,75.5 278.3,72.3 279,69.2 279.9,66.3 280.7,63.7 281.5,61.2 282.3,58.9 283.1,56.9 284,55.1 284.8,53.5 285.6,52.2 286.4,51.1 287.2,50.2 288,49.6 288.8,49.2 289.6,49.1 290.4,49.2 291.2,49.6 292,50.2 292.9,51.1 293.7,52.2 294.5,53.5 295.3,55.1 296.1,56.9 297,58.9 297.7,61.2 298.5,63.7 299.3,66.3 300.1,69.2 301,72.3 301.8,75.5 302.6,78.9 303.4,82.5 304.2,86.2 305,90.1 305.8,94.1 306.6,98.2 307.4,102.5 308,106.8 309,111.3 309.9,115.8 310.7,120.4 311.5,125 312.3,129.7 313.1,134.5 314,139.2 314.7,144 315.5,148.8 316.3,153.6 317.2,158.3 318,163 318.8,167.8 319.6,172.5 320.4,177.1 321.2,181.7 322,186.3 322.8,190.7 323.6,195.1 324.4,199.4 325.3,203.7 326,207.8 326.9,211.9 327.7,215.8 328.5,219.7 329.3,223.4 330.1,227.1 331,230.7 331.7,234.1 332.5,237.4 333.4,240.7 334.2,243.8 335,246.8 335.8,249.7 336.6,252.4 337.4,255.1 338.2,257.7 339,260.1 339.8,262.5 340.6,264.7 341.5,266.9 342.3,268.9 343,270.9 343.9,272.7 344.7,274.5 345.5,276.2 346.3,277.8 347.1,279.2 347.9,280.7 348.7,282 349.6,283.3 350.4,284.5 351.2,285.6 352,286.6 352.8,287.6 353.6,288.5 354.4,289.4 355.2,290.2 356,291 356.8,291.7 357.7,292.3 358.5,293 359.3,293.5 360,294 360.9,294.5 361.7,295 362.5,295.4 363.3,295.8 364.1,296.2 365,296.5 365.8,296.8 366.6,297.1 367.4,297.4 368.2,297.6 369,297.8 369.8,298 370.6,298.2 371.4,298.4 372.2,298.5 373,298.7 373.9,298.8 374.7,298.9 375.5,299 376.3,299.1 377.1,299.2 378,299.3 378.7,299.4 379.5,299.4 380.3,299.5 381.1,299.5 382,299.6 382.8,299.6 383.6,299.7 384.4,299.7 385.2,299.7 386,299.8 388.4,299.8 389.2,299.9 395.7,299.9 396.5,300 545.6,300" stroke-width="2" stroke="blue" fill="url(#d)"/><path d="M221,303v-257h2v257z" fill="green"/><path d="M356,303v-257h2v257z" fill="green"/><path d="M290,303v-8h2v8z" fill="black"/><text font-family="KaTeX_Main, serif" font-size="25" x="290" y="232" text-anchor="middle">1-α</text></svg></center>
> 
> $17\notin I_\mu\implies$Vi förkastar $H_0$ *på risknivån 1%*.
> 
> Om risknivån är $\alpha=0.001$ blir $t_{\alpha/2}(n-1)=t_{0.0005}(59)$, och<br>
> $17\in I_\mu\implies$Vi kan inte förkasta $H_0$ *på nivån 0,1%*.
> 
> *p-värdet:* $P(\text{förkasta }H_0|H_0\text{ sann \& mätdata})\\=2\cdot(\text{area t.h. om 17})=\left\{\begin{matrix}\lt0.01\\\text{men}\\\gt0.001\end{matrix}\right\}\\\implies H_0$ kan förkastas med felrisk 0.01, men inte med felrisk 0.001.


#### Testvariabelmetoden (liknande idé)
> **Exempel:** Samma exempel som ovan.<br>
> Bilda $Y=\frac{\bar X-\mu}{S/\sqrt n}\in t(n-1)$.<br>
> Tvåsidigt test<br>
> $t_\text{obs}=\left|\frac{\bar x-\mu}{s/\sqrt n}\right|=3.1$ jämförs med $t_{\alpha/2}(n-1)$<br>
> 1%: $t_{0.01/2}(59)=2.66\implies P(Y\gt2.66)=0.005$<br>
> 0.1%: $t_{0.001/2}(59)=3.46\implies P(Y\gt3.46)=0.0005$<br>
> $t_\text{obs}\gt2.66\implies H_0$ förkastas på nivån 1%.<br>
> $t_\text{obs}\lt3.46\implies H_0$ förkastas inte på nivån 0,1%!<br>
> 
> $P(\text{förkastas }H_0|H_0\text{ sant})\lt\alpha\implies H_0$ kan förkastas<br>
> p-värdet = 2 * (arean t.h. om 3.1) = 0.003<br>
> Dvs. vi kan förkasta $H_0$ på risknivån 1%, men inte på nivån 0,1%.

## Föreläsning 14
*Videoföreläsning #14 av Björn-Olof Skytt.*

### Ensidiga test
$X_i$ antas oberoende och $N(\mu,\sigma)$. Vi har 60 observationer och har fått $\bar x=16.51$ och $\frac s{\sqrt n}=0.159$.<br>
Vi ställer likt tidigare upp nollhypotesen $H_0:\mu=17$.

***Till skillnad från tidigare har vi dock en ensidig mothypotes:*** $H_1:\mu\lt17$

Observera att mothypotesen $H_1:\mu\lt17$ bara är intressant om det praktiska värdet ($\bar x$) är mindre än 17, eftersom vi annars aldrig kan förkasta $H_0$ *till förmån för* $H_1$.

För att lösa detta behöver vi ställa upp ett ensidigt konfidensintervall, dvs. där hela felrisken $\alpha$ är på ena sidan. Vi utgår från §12.2 men sätter den vänstra gränsen till $-\infty$ och "flyttar" den vänstra "ytterkantsarean" till höger sida. Vårt ensidiga konfidensintervall blir då

$$I_\mu=(-\infty,\bar x+\frac s{\sqrt n}t_\alpha(n-1))$$

Vi bestämmer detta till $I_\mu=(-\infty,16.51+0.38)\notni17\\\implies$ Vi förkastar $H_0$ på risknivån 1 %.

#### Testvariabelmetoden
§11.1d: $\frac{\bar X-\mu}{S/\sqrt n}\in t(n-1)$<br>
$t_\text{obs}=\left|\frac{\bar x-\mu}{s/\sqrt n}\right|=\left|\frac{16.51-17}{0.159}\right|=3.1$ jämförs med $t_\alpha(n-1)$.

### Styrkefunktion
> **Hur beräknas *testets* styrka m.h.a. konfidensmetoden?**<br>
> §14 Styrkefunktionen $h(\theta)=P(\text{förkasta }H_0|\theta\text{ är rätt värde})$
> 
> ***Styrkan hos testet*** för $H_1:\theta=\theta_1$ är
> 
> $$P(\text{förkasta }H_0:\theta=\theta_0|H_1:\theta=\theta_1\text{ är sann})$$
> 
> *Exempel (13.21a):*<br>
> $X_i$ oberoende och $N(\mu_x,0.3)$, och<br>
> $Y_i$ oberoende  och $N(\mu_y,0.4)$. Vi gör $n_x=n_y=10$ försök.<br>
> $H_0:\mu_x=\mu_y\iff\mu_x-\mu_y=0$<br>
> $H_1:\mu_x-\mu_y=0.6$<br>
> Låt risknivån $\alpha=.01$.
> 
> Vi bildar konfidensintervall för $\mu_x-\mu_y$ m.h.a. §12.1 och §11.3:<br>
> $I_{\mu_x-\mu_y}=\bar x-\bar y\pm\sqrt{\frac{0.3^2}{10}+\frac{0.4^2}{10}}\lambda_{\alpha/2}=\bar x-\bar y\pm0.407$<br>
> Styrkan hos test för $H_1:\mu_x-\mu_y=0.6$:<br>
> $P(\text{förkasta }H_0:\mu_x-\mu_y=0|H_1:\mu_x-\mu_y=0.6\text{ är sann})\\=P(0\notin\text{motsv. konf.intv. för }\mu_x-\mu_y|H_1\text{ är sann})\\=P(|(\bar X-\bar Y)-0|>0.407|H_1\text{ är sann})\\=1-P(-0.407\lt\bar X-\bar Y\lt0.407|H_1\text{ är sann})\\=\{H_1\text{ är sann}\implies\mu_x-\mu_y=0.6\}\\=1-P\left(\frac{-0.407-0.6}{\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}}\lt\frac{(\bar X-\bar Y)-0.6}{\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}}\lt\frac{0.407-0.6}{\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}}\right)\\=1-(\Phi\left(\frac{-0.193}{\sqrt{\frac{{0.3}^2}{10}+\frac{{0.4}^2}{10}}}\right)-\Phi\left(\frac{-1.007}{\sqrt{\frac{{0.3}^2}{10}+\frac{{0.4}^2}{10}}}\right))=0.886$
> 
> *Styrkefunktionen* är enligt härledningen ovan (låt $\Delta=\mu_x-\mu_y$):
> 
> $$h(\Delta)=1-(\Phi\left(\frac{0.407-\Delta}{\sqrt{\frac{{0.3}^2}{10}+\frac{{0.4}^2}{10}}}\right)-\Phi\left(\frac{-0.407-\Delta}{\sqrt{\frac{{0.3}^2}{10}+\frac{{0.4}^2}{10}}}\right))$$

### Linjär regression
Vi tänker oss att ett samband inte är perfekt, utan att, för varje $x_i$ kommer

$$Y_i=\alpha+\beta y+\epsilon$$

där $\epsilon\in N(0,\sigma)$

Skillnaderna $y_i-(\alpha+\beta x_i)$, dvs. skillnaderna mellan praktiskt värde $y_i$ och teoretiskt värde (väntevärde) $\alpha+\beta x_i$, kallas för *residualer*.

Vi kan ta fram (*skatta*) modellkoefficienterna $\alpha,\beta$ med minsta kvadratmetoden:

$$Q=\sum_{i=1}^n\left(y_i-(\alpha+\beta x_i)\right)^2$$

$\frac{\partial Q}{\partial\alpha}=0,\frac{\partial Q}{\partial\beta}=0\implies\begin{cases}\alpha_\text{obs}^*=\bar y-\beta_\text{obs}^*\bar x\\\beta_\text{obs}^*=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^n(x_i-\bar x)^2}\end{cases}$

#### Hypotesprövning
Om man vill testa om $y$ beror av $x$ gör man följande hypotesprövning:

$$H_0:\beta=0,\ H_1:\beta\neq0$$

Om vi kan förkasta $H_0$ så antar vi att $y$ beror av $x$.<br>
Om vi *inte* kan förkasta $H_0$, så *antar vi att* $y$ inte beror av $x$
> *Observera att detta antagande inte görs i vanlig hypotesprövning. I dessa fall vet vi sedan tidigare att:*<br>
> Icke-signifikant resultat (*förkasta inte*)$\rlap{\(\quad\not\)}\implies H_0$ sant

**Hur undersöker vi detta?** *(§13.2)*

$$I_\beta=\beta_\text{obs}^*\pm t_{p/2}(n-2)\frac s{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}}$$

*Obs!* $s^2=\frac1{n-2}\sum_{i=1}^n(y_i-(\alpha_\text{obs}^*+\beta_\text{obs}^*x_i))^2$

#### Multipel regression
$$Y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots$$

> **Övningsuppgift 14.7:**<br>
> $y=$ Nikotin, $x_1=$ Kol, $x_2=$ Klor<br>
> $y=\alpha+\beta_1x_1+\beta_2x_2$
> 
> ||Koefficient|Standardfel|p-värde|Nedre 95%|Övre 95%
> |-|-|-|-|-|-|
> |Intercept|$-0.471$|$0.626$|$0.459$|$-1.769$|$0.826$
> |Kol|$1.423$|$0.247$|$8.3\cdot10^{-6}$|$0.912$|$1.935$
> |Klor|$-0.176$|$0.112$|$0.132$|$-0.409$|$0.057$
> 
> a) Ska hypoteserna $H_{\beta_1}:\beta_1=0$ respektive $H_{\beta_2}:\beta_2=0$ förkastas på 5% signifikansnivå?<br>
> Vi ser i tabellen att $0\notin I_{\beta_1}\implies$Vi förkastar $H_{\beta_1}$<br>
> Vi ser dock att $0\in I_{\beta_2}\implies$Vi kan ej förkasta $H_{\beta_2}$
> 
> b) ...
> 
> c) Beräkna medelfelet till skattningen i b).<br>
> $D_\text{obs}^*({\beta_1}^*+\beta_2^*)$<br>
> Vi beräknar<br>
> $V_\text{obs}^*(\beta_1^*+\beta_2^*)=V_\text{obs}^*(\beta_1^*)+V_\text{obs}^*(\beta_2^*)+2\cdot C_\text{obs}^*(\beta_1^*,\beta_2^*)\\={0.247}^2+{0.112}^2+2\cdot0.00248=0.0785$<br>
> $\implies$Medelfelet $=\sqrt{0.0785}=0.28$

#### Residualanalys
Används för att avgöra om vi verkligen kan anpassa en rät linje till våra punkter.

Om vi ser en tendens hos residualerna$\implies$inte rät linje.<br>
Om residualerna avviker mer och mer från 0, och åt olika håll, indikerar detta att de inte har samma varians$\implies$troligen ej rät linje.

## Föreläsning 15
*Videoföreläsning #15 av Björn-Olof Skytt.*

### $\chi^2$-test: (1) Test av given fördelning
Används när *nollhypotesen är en given sannolikhetsfunktion*.<br>
Dvs. $H_0:P(A_1)=p_1,P(A_2)=p2,\dots$ $\left(\sum=1\right)$

Vi använder §14.3. $r$ är antalet distinkta utfall $A_1,\dots,A_r$.

$$Q=\sum_{i=1}^r\frac{(x_i-np_i)^2}{np_i}$$

där $x_i$ är antalet gånger vi får resultatet $A_i$.

*"Om $Q$ är stort så förkastas $H_0$ på risknivån $\alpha$."*<br>
Enligt formelsamlingen är Q ett utfall av $\chi^2(r-1)$<br>
**När?** Då $Q\gt\chi^2_\alpha(f)$.
> **Liten bakgrund till $Q$:**<br>
> $Q$ är egentligen en *observation* av en stokastisk variabel.<br>
> $Q_\text{s.v.}=\sum_{i=1}^r\left(\frac{X_i-np_i}{\sqrt{np_i}}\right)^2=\sum_{i=1}^nY_i^2$<br>
> $Y_i$:na är *approximativt* $N(0,1)$ om alla $np_i\geq5$<br>
> $\implies Q$ approximativt $\chi^2$-fördelat (se §10)

> **Exempel:** Misstänker fusk vid roulettebord. Vi spelar 8000 ggr.<br>
> Korrekt borde röd:svart:noll = 18:18:1.<br>
> Testresultat: 3751:4018:231<br>
> Undersök om rouletten är korrekt med $\alpha=1\%$ felrisk.
> 
> $H_0:P(\text{röd})=P(\text{svart})=18/37,P(\text{noll})=1/37$<br>
> Villkor för $\chi^2$-test är att alla $np_i\geq5$. Vi får:<br>
> $n\min p_i=8000\frac{18}{37}\geq5$ $\therefore$ *OK*<br>
> $Q=\sum_{i=1}^r\frac{(x_i-np_i)^2}{np_i}\\=\frac{\left(3751-8000\cdot \frac{18}{37}\right)^2}{8000\cdot \frac{18}{37}}+\frac{\left(4018-8000\cdot \frac{18}{37}\right)^2}{8000\cdot \frac{18}{37}}+\frac{\left(231-8000\cdot \frac1{37}\right)^2}{8000\cdot \frac1{37}}\approx10.20$<br>
> $\alpha=P(H_0\text{ förkastas }|H_0\text{ sann})=P(Q_\text{s.v.}\gt\chi^2_\alpha(f))$<br>
> Vi har det kritiska området $C=\{\gt\chi^2_{0.01}(3-1)=9.21\}$.<br>
> $Q\in C\implies$Vi förkastar $H_0:\text{"spelet rättvist"}$ med felrisk $1\%$.

#### Svårt exempel
> **Exempel 13.18:**<br>
> I en klassisk datamängd undersöktes antalet ihjälsparkade soldater vid 14 tyska armékårer från 1875 till 1894 (20 år). De $14\cdot20=260$ rapporterna fördelade sig som i tabellen.
> 
> |Antal döda|Antal rapporter|Andel|
> |-|-|-|
> |0|144|0,5143|
> |1|91|0,3250|
> |2|32|0,1143|
> |3|11|0,0393|
> |4|2|0,0071|
> |&#8805;5|0|0|
> |**Summa**|280|1|
> 
> Totalt dog 196 soldater. Låt $Y=\text{"antal döda i en rapport"}$.<br>
> *Man vill nu testa $H_0:Y\in P_o(\mu)$*<br>
> $n=280,r=6$<br>
> Enligt nollhypotesen är sannolikheterna:<br>
> $p_0=P(0\text{ döda})=\frac{\mu^0}{0!}e^{-\mu}=e^{-\mu}$<br>
> $p_1=P(1\text{ död})=\frac{\mu^1}{1!}e^{-\mu}=\mu e^{-\mu}$<br>
> ...
> $p_5=P(\geq5\text{ döda})=1-\sum_{i=0}^4p_i$
> 
> Förutsättning: $\min(np_i)=\{\text{givet }H_0\}=\geq5$
> 
> $$Q=\sum_{i=0}^5\frac{(x_i-np_i)^2}{np_i}$$
> 
> $\mu$ ges ej av nollhypotesen, och måste därför skattas ur data:<br>
> $\mu=\mu_\text{obs}^*=\bar y=\frac{196}{280}=0.7$
> 
> $\implies n{p_1}_\text{obs}^*=280\cdot e^{-\mu_\text{obs}^*}=280\cdot e^{-0.7}$ osv...<br>
> *Om vi fortsätter att skatta alla $np_i$ ser vi att vissa $\lt5$.<br>
> Vi måste **slå samman grupper**:* Låt sista gruppen vara $P(\geq3\text{ döda})$.<br>
> Nu uppfyller alla $n{p_i}\geq5$.<br>
> *Istället för 6 utfall, har vi nu istället $r'=4$ st:* $p_0,p_1,p_2,p_3$<br>
> $x_0=144,x_1=91,x_2=32,x_3=11+2+0=13$<br>
> Vi beräknar nu $Q'=\sum_{i=0}^5\frac{(x_i-n{p_i}_\text{obs}^*)^2}{n{p_i}_\text{obs}^*}=1.95$.
> 
> Observera att vi nu ska jämföra med $\chi^2_\alpha(r'-k-1)$.<br>
> Vi har skattat 1 variabler ur data, dvs. $k=1$.<br>
> $\chi^2_\alpha(4-1-1)=\chi^2_{0.05}(2)=5.99$<br>
> **Svar:** Eftersom $Q'\lt5.99\implies$Vi kan **inte** förkasta $H_0:Y\in P_o(\mu)$ på risknivån $5\%$.

### $\chi^2$-test: (2) Homogenitetstest
Används när nollhypotesen är att vi har *samma sannolikhetsfunktion hos olika grupper*. T.ex. jämföra fördelning över 4 utfall (oskadad, lätt, svårt, död) vid bilkrock, för personer med resp. utan bilbälte. Då är *nollhypotesen* att fördelningarna är lika, dvs. att bilbälte inte påverkar.

$$Q=\sum_{i=1}^s\sum_{j=1}^r\frac{\left(x_{ij}-\frac{n_im_j}N\right)^2}{\frac{n_im_j}N}$$

Villkor för homogentitetstest: $\frac{n_im_j}N\geq5$

Om $Q\gt\chi^2_\alpha((r-1)(s-1))$ förkastas $H_0$.

> **Exempel (tentauppgift aug 2018):**<br>
> En komponent kan orienteras på ett kretskort i två lägen: A-/B-läge
> 
> ||Fungerar<br>($j=1$)|Fel 1<br>($j=2$)|Fel 2<br>($j=3$)|
> |-|-|-|-|
> |A-läge ($i=1$)|27|15|8|
> |B-läge $(i=2)$|22|7|21|
> 
> Undersök om monteringsläget har något betydelse med $\alpha=0.05$.<br>
> $H_0:\text{"sannolikhetsfunktionen är samma för A- \& B-läge"}$<br>
> $Q=\sum_{i=1}^2\sum_{j=1}^3\frac{\left(x_{ij}-\frac{n_im_j}N\right)^2}{\frac{n_im_j}N}$<br>
> ($s=2$ grupper, $r=3$ resultat)<br>
> $n_1=27+15+8=50,n_2=22+7+21=50$<br>
> $m_1=27+22=49,m_2=15+7=22,m_3=8+21=29$<br>
> $N=\sum n_i=\sum m_j=100$<br>
> Vi kollar att villkoret $\min\frac{n_im_j}N=\frac{50\cdot22}{100}\geq5\implies$OK<br>
> $Q=\frac{\left(27-\frac{50\cdot49}{100}\right)^2}{\frac{50\cdot49}{100}}+\frac{\left(15-\frac{50\cdot22}{100}\right)^2}{\frac{50\cdot22}{100}}+\cdots+\frac{\left(21-\frac{50\cdot29}{100}\right)^2}{\frac{50\cdot29}{100}}=9.2469$<br>
> Ska jämföras med $\chi^2_\alpha((r-1)(s-1))=\chi^2_{0.05}(2)=5.99$<br>
> $Q\gt\chi_\alpha^2\implies$Förkasta $H_0$ (dvs. A-/B-läge har betydelse)

<!-- -->
> **På räknaren finns en funktion för $\chi^2$-test. Denna kan användas för att göra *homogenitetstest*.**

### $\chi^2$-test: (3) Oberoendetest
Används för att undersöka om två egenskaper A och B är oberoende, där A kan ha utfallen $A_1,\dots,A_r$ och B kan ha utfallen $B_1,\dots,B_s$.<br>
Nollhypotesen är då $H_0:$ A & B oberoende

Beräkningsmässigt används samma metod som för homogenitetstest.

> **Exempel (tentauppgift jan 2017):**<br>
> Vi vill undersöka om vintertemperaturerna på Grönland samvarierar med motsvarande temperaturer i Danmark.
> 
> ||Kall vinter<br>Nuuk|Normal vinter<br>Nuuk|Mild vinter<br>Nuuk|
> |-|-|-|-|
> |<b>Kall<br>vinter<br>Köpenhamn</b>|2|19|9|
> |<b>Normal<br>vinter<br>Köpenhamn</b>|13|51|22|
> |<b>Mild<br>vinter<br>Köpenhamn</b>|12|18|2|
> 
> $H_0:\text{"vintertemperaturerna är oberoende"}$<br>
> Vi gör i detta fall ett oberoendetest som rent praktiskt utförs som ett homogenitetstest.<br>
> Kolla att alla $\min\frac{n_im_j}N\geq5$. Efter beräkning ser vi att detta villkor är uppfyllt.<br>
> Vi beräknar nu
> 
> $$Q=\sum_{i=1}^2\sum_{j=1}^3\frac{\left(x_{ij}-\frac{n_im_j}N\right)^2}{\frac{n_im_j}N}=14.21$$
> 
> Vi jämför detta med $\chi^2_\alpha((3-1)(3-1))=\chi^2_{0.01}(4)=13.28$<br>
> Vi ser att $Q\gt\chi^2_\alpha(2)\implies$Vi förkastar att vintertemperaturerna på Grönland och Danmark är oberoende.
> 
> **p-värde**: = 2nd-fördelning = $\chi^2_\text{cdf}(14.21,10^{99},4)=0.006654$


**Slut på sista föreläsningen!**

